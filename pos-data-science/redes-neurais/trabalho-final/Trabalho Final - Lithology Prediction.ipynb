{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Habilitar cuda se ela estiver disponível\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lithology = pd.read_csv('lithology.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>RDEP</th>\n",
       "      <th>RHOB</th>\n",
       "      <th>GR</th>\n",
       "      <th>...</th>\n",
       "      <th>Carbon_Index</th>\n",
       "      <th>Normalized_RHOB</th>\n",
       "      <th>Normalized_GR</th>\n",
       "      <th>Delta_DTC</th>\n",
       "      <th>Delta_RHOB</th>\n",
       "      <th>Delta_GR</th>\n",
       "      <th>Delta_DEPTH_MD</th>\n",
       "      <th>Delta_Carbon_Index</th>\n",
       "      <th>GROUP_encoded</th>\n",
       "      <th>FORMATION_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494.528</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.501831</td>\n",
       "      <td>19.480835</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.611410</td>\n",
       "      <td>1.798681</td>\n",
       "      <td>1.884186</td>\n",
       "      <td>80.200851</td>\n",
       "      <td>...</td>\n",
       "      <td>24.735691</td>\n",
       "      <td>0.314847</td>\n",
       "      <td>0.150172</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494.680</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.653809</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.618070</td>\n",
       "      <td>1.795641</td>\n",
       "      <td>1.889794</td>\n",
       "      <td>79.262886</td>\n",
       "      <td>...</td>\n",
       "      <td>24.492376</td>\n",
       "      <td>0.318528</td>\n",
       "      <td>0.148269</td>\n",
       "      <td>0.527710</td>\n",
       "      <td>-0.005608</td>\n",
       "      <td>0.937965</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.243315</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>494.832</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.805786</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.626459</td>\n",
       "      <td>1.800733</td>\n",
       "      <td>1.896523</td>\n",
       "      <td>74.821999</td>\n",
       "      <td>...</td>\n",
       "      <td>24.202299</td>\n",
       "      <td>0.322946</td>\n",
       "      <td>0.139258</td>\n",
       "      <td>0.429855</td>\n",
       "      <td>-0.006729</td>\n",
       "      <td>4.440887</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-0.290077</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>494.984</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.957794</td>\n",
       "      <td>19.459282</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.621594</td>\n",
       "      <td>1.801517</td>\n",
       "      <td>1.891913</td>\n",
       "      <td>72.878922</td>\n",
       "      <td>...</td>\n",
       "      <td>24.400797</td>\n",
       "      <td>0.319919</td>\n",
       "      <td>0.135315</td>\n",
       "      <td>0.024185</td>\n",
       "      <td>0.004610</td>\n",
       "      <td>1.943077</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.198498</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>495.136</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.109772</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.602679</td>\n",
       "      <td>1.795299</td>\n",
       "      <td>1.880034</td>\n",
       "      <td>71.729141</td>\n",
       "      <td>...</td>\n",
       "      <td>24.916765</td>\n",
       "      <td>0.312121</td>\n",
       "      <td>0.132982</td>\n",
       "      <td>0.021088</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>1.149780</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.515968</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEPTH_MD         X_LOC      Y_LOC       Z_LOC       CALI   RSHA      RMED  \\\n",
       "0   494.528  437641.96875  6470972.5 -469.501831  19.480835 -999.0  1.611410   \n",
       "1   494.680  437641.96875  6470972.5 -469.653809  19.468800 -999.0  1.618070   \n",
       "2   494.832  437641.96875  6470972.5 -469.805786  19.468800 -999.0  1.626459   \n",
       "3   494.984  437641.96875  6470972.5 -469.957794  19.459282 -999.0  1.621594   \n",
       "4   495.136  437641.96875  6470972.5 -470.109772  19.453100 -999.0  1.602679   \n",
       "\n",
       "       RDEP      RHOB         GR  ...  Carbon_Index  Normalized_RHOB  \\\n",
       "0  1.798681  1.884186  80.200851  ...     24.735691         0.314847   \n",
       "1  1.795641  1.889794  79.262886  ...     24.492376         0.318528   \n",
       "2  1.800733  1.896523  74.821999  ...     24.202299         0.322946   \n",
       "3  1.801517  1.891913  72.878922  ...     24.400797         0.319919   \n",
       "4  1.795299  1.880034  71.729141  ...     24.916765         0.312121   \n",
       "\n",
       "   Normalized_GR  Delta_DTC  Delta_RHOB  Delta_GR  Delta_DEPTH_MD  \\\n",
       "0       0.150172  -0.000000   -0.000000 -0.000000           0.000   \n",
       "1       0.148269   0.527710   -0.005608  0.937965           0.152   \n",
       "2       0.139258   0.429855   -0.006729  4.440887           0.152   \n",
       "3       0.135315   0.024185    0.004610  1.943077           0.152   \n",
       "4       0.132982   0.021088    0.011879  1.149780           0.152   \n",
       "\n",
       "   Delta_Carbon_Index  GROUP_encoded  FORMATION_encoded  \n",
       "0            0.000000              6                 68  \n",
       "1           -0.243315              6                 68  \n",
       "2           -0.290077              6                 68  \n",
       "3            0.198498              6                 68  \n",
       "4            0.515968              6                 68  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lithology.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rápido EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000.0    720803\n",
       "30000.0    168937\n",
       "65030.0    150455\n",
       "70000.0     56320\n",
       "80000.0     33329\n",
       "99000.0     15245\n",
       "70032.0     10513\n",
       "88000.0      8213\n",
       "90000.0      3820\n",
       "74000.0      1688\n",
       "86000.0      1085\n",
       "93000.0       103\n",
       "Name: FORCE_2020_LITHOFACIES_LITHOLOGY, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lithology['FORCE_2020_LITHOFACIES_LITHOLOGY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_lithology['FORCE_2020_LITHOFACIES_LITHOLOGY'].value_counts().plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base contém um desbalanceamento. Será lidado com ele usando estratégias de split estratificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
       "       'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
       "       'MUDWEIGHT', 'RMIC', 'FORCE_2020_LITHOFACIES_LITHOLOGY', 'Carbon_Index',\n",
       "       'Normalized_RHOB', 'Normalized_GR', 'Delta_DTC', 'Delta_RHOB',\n",
       "       'Delta_GR', 'Delta_DEPTH_MD', 'Delta_Carbon_Index', 'GROUP_encoded',\n",
       "       'FORMATION_encoded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lithology.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As colunas que contém no nome _Normalized_ ou _Delta_ já devem ter passado por algum processamento. A coluna _FORCE_2020_LITHOFACIES_LITHOLOGY_ é o que se quer prever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>RDEP</th>\n",
       "      <th>RHOB</th>\n",
       "      <th>GR</th>\n",
       "      <th>...</th>\n",
       "      <th>Carbon_Index</th>\n",
       "      <th>Normalized_RHOB</th>\n",
       "      <th>Normalized_GR</th>\n",
       "      <th>Delta_DTC</th>\n",
       "      <th>Delta_RHOB</th>\n",
       "      <th>Delta_GR</th>\n",
       "      <th>Delta_DEPTH_MD</th>\n",
       "      <th>Delta_Carbon_Index</th>\n",
       "      <th>GROUP_encoded</th>\n",
       "      <th>FORMATION_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.081904</td>\n",
       "      <td>0.249745</td>\n",
       "      <td>-0.987890</td>\n",
       "      <td>0.242287</td>\n",
       "      <td>0.072961</td>\n",
       "      <td>0.055864</td>\n",
       "      <td>0.062862</td>\n",
       "      <td>0.267399</td>\n",
       "      <td>0.098282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252801</td>\n",
       "      <td>0.267160</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.215984</td>\n",
       "      <td>0.266868</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.266868</td>\n",
       "      <td>0.056807</td>\n",
       "      <td>-0.293998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X_LOC</th>\n",
       "      <td>-0.081904</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.416875</td>\n",
       "      <td>0.085877</td>\n",
       "      <td>-0.249819</td>\n",
       "      <td>-0.164608</td>\n",
       "      <td>-0.044303</td>\n",
       "      <td>0.054780</td>\n",
       "      <td>0.098533</td>\n",
       "      <td>0.231190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096850</td>\n",
       "      <td>0.098485</td>\n",
       "      <td>0.167773</td>\n",
       "      <td>-0.071248</td>\n",
       "      <td>0.098435</td>\n",
       "      <td>-0.000287</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.098434</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>-0.225151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y_LOC</th>\n",
       "      <td>0.249745</td>\n",
       "      <td>0.416875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.259623</td>\n",
       "      <td>-0.148181</td>\n",
       "      <td>-0.175650</td>\n",
       "      <td>0.109941</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>0.106056</td>\n",
       "      <td>0.206453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102346</td>\n",
       "      <td>0.105965</td>\n",
       "      <td>0.131760</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>0.024585</td>\n",
       "      <td>-0.163860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z_LOC</th>\n",
       "      <td>-0.987890</td>\n",
       "      <td>0.085877</td>\n",
       "      <td>-0.259623</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.234391</td>\n",
       "      <td>-0.088610</td>\n",
       "      <td>-0.087831</td>\n",
       "      <td>-0.111010</td>\n",
       "      <td>-0.277483</td>\n",
       "      <td>-0.101529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262971</td>\n",
       "      <td>-0.277247</td>\n",
       "      <td>-0.043703</td>\n",
       "      <td>-0.208389</td>\n",
       "      <td>-0.276951</td>\n",
       "      <td>-0.000414</td>\n",
       "      <td>-0.002509</td>\n",
       "      <td>-0.276951</td>\n",
       "      <td>-0.044176</td>\n",
       "      <td>0.291771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CALI</th>\n",
       "      <td>0.242287</td>\n",
       "      <td>-0.249819</td>\n",
       "      <td>-0.148181</td>\n",
       "      <td>-0.234391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230859</td>\n",
       "      <td>0.200187</td>\n",
       "      <td>0.059856</td>\n",
       "      <td>0.290623</td>\n",
       "      <td>-0.131479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288104</td>\n",
       "      <td>0.290592</td>\n",
       "      <td>-0.133204</td>\n",
       "      <td>0.394364</td>\n",
       "      <td>0.290467</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.290466</td>\n",
       "      <td>0.055227</td>\n",
       "      <td>0.017484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RSHA</th>\n",
       "      <td>0.072961</td>\n",
       "      <td>-0.164608</td>\n",
       "      <td>-0.175650</td>\n",
       "      <td>-0.088610</td>\n",
       "      <td>0.230859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038447</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>0.113860</td>\n",
       "      <td>-0.043932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112537</td>\n",
       "      <td>0.113915</td>\n",
       "      <td>-0.063521</td>\n",
       "      <td>0.088760</td>\n",
       "      <td>0.113818</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>-0.001663</td>\n",
       "      <td>0.113820</td>\n",
       "      <td>-0.011004</td>\n",
       "      <td>0.001706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMED</th>\n",
       "      <td>0.055864</td>\n",
       "      <td>-0.044303</td>\n",
       "      <td>0.109941</td>\n",
       "      <td>-0.087831</td>\n",
       "      <td>0.200187</td>\n",
       "      <td>0.038447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093780</td>\n",
       "      <td>0.204210</td>\n",
       "      <td>0.117150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204782</td>\n",
       "      <td>0.204219</td>\n",
       "      <td>0.087406</td>\n",
       "      <td>0.350722</td>\n",
       "      <td>0.204328</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.204323</td>\n",
       "      <td>-0.031007</td>\n",
       "      <td>-0.022347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RDEP</th>\n",
       "      <td>0.062862</td>\n",
       "      <td>0.054780</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>-0.111010</td>\n",
       "      <td>0.059856</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>0.093780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113988</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114447</td>\n",
       "      <td>0.113994</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.090665</td>\n",
       "      <td>0.114003</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.114004</td>\n",
       "      <td>0.045048</td>\n",
       "      <td>-0.006239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RHOB</th>\n",
       "      <td>0.267399</td>\n",
       "      <td>0.098533</td>\n",
       "      <td>0.106056</td>\n",
       "      <td>-0.277483</td>\n",
       "      <td>0.290623</td>\n",
       "      <td>0.113860</td>\n",
       "      <td>0.204210</td>\n",
       "      <td>0.113988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062674</td>\n",
       "      <td>0.462418</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.022361</td>\n",
       "      <td>-0.127749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GR</th>\n",
       "      <td>0.098282</td>\n",
       "      <td>0.231190</td>\n",
       "      <td>0.206453</td>\n",
       "      <td>-0.101529</td>\n",
       "      <td>-0.131479</td>\n",
       "      <td>-0.043932</td>\n",
       "      <td>0.117150</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006083</td>\n",
       "      <td>-0.002662</td>\n",
       "      <td>0.583831</td>\n",
       "      <td>-0.074397</td>\n",
       "      <td>-0.002677</td>\n",
       "      <td>-0.051292</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>-0.002682</td>\n",
       "      <td>0.062451</td>\n",
       "      <td>-0.172180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPHI</th>\n",
       "      <td>0.414375</td>\n",
       "      <td>0.089433</td>\n",
       "      <td>0.068874</td>\n",
       "      <td>-0.411152</td>\n",
       "      <td>0.159466</td>\n",
       "      <td>0.108301</td>\n",
       "      <td>0.087206</td>\n",
       "      <td>0.086625</td>\n",
       "      <td>0.474023</td>\n",
       "      <td>0.169099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467362</td>\n",
       "      <td>0.473941</td>\n",
       "      <td>0.091346</td>\n",
       "      <td>0.250326</td>\n",
       "      <td>0.473603</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.473602</td>\n",
       "      <td>0.063828</td>\n",
       "      <td>-0.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEF</th>\n",
       "      <td>0.201044</td>\n",
       "      <td>-0.035485</td>\n",
       "      <td>0.071257</td>\n",
       "      <td>-0.197759</td>\n",
       "      <td>0.253697</td>\n",
       "      <td>-0.048833</td>\n",
       "      <td>0.196880</td>\n",
       "      <td>0.014515</td>\n",
       "      <td>0.458521</td>\n",
       "      <td>0.049547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457744</td>\n",
       "      <td>0.458447</td>\n",
       "      <td>-0.063188</td>\n",
       "      <td>0.213564</td>\n",
       "      <td>0.458318</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.458312</td>\n",
       "      <td>0.024317</td>\n",
       "      <td>-0.007112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTC</th>\n",
       "      <td>0.086242</td>\n",
       "      <td>-0.076769</td>\n",
       "      <td>0.030885</td>\n",
       "      <td>-0.081243</td>\n",
       "      <td>0.336849</td>\n",
       "      <td>0.068859</td>\n",
       "      <td>0.354425</td>\n",
       "      <td>0.076234</td>\n",
       "      <td>0.424886</td>\n",
       "      <td>-0.077324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428413</td>\n",
       "      <td>0.424933</td>\n",
       "      <td>-0.019223</td>\n",
       "      <td>0.973028</td>\n",
       "      <td>0.424812</td>\n",
       "      <td>-0.000272</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.424818</td>\n",
       "      <td>0.028162</td>\n",
       "      <td>-0.028480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP</th>\n",
       "      <td>-0.212204</td>\n",
       "      <td>-0.121053</td>\n",
       "      <td>-0.076501</td>\n",
       "      <td>0.174611</td>\n",
       "      <td>0.117929</td>\n",
       "      <td>0.262062</td>\n",
       "      <td>0.248956</td>\n",
       "      <td>0.023633</td>\n",
       "      <td>0.074638</td>\n",
       "      <td>-0.161919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079151</td>\n",
       "      <td>0.074799</td>\n",
       "      <td>-0.089115</td>\n",
       "      <td>0.170855</td>\n",
       "      <td>0.074868</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.002900</td>\n",
       "      <td>0.074867</td>\n",
       "      <td>-0.072891</td>\n",
       "      <td>0.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BS</th>\n",
       "      <td>-0.618097</td>\n",
       "      <td>-0.092630</td>\n",
       "      <td>-0.167017</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>-0.093683</td>\n",
       "      <td>-0.077242</td>\n",
       "      <td>-0.084409</td>\n",
       "      <td>-0.049338</td>\n",
       "      <td>-0.336723</td>\n",
       "      <td>-0.182325</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.327054</td>\n",
       "      <td>-0.336593</td>\n",
       "      <td>-0.086266</td>\n",
       "      <td>-0.194345</td>\n",
       "      <td>-0.336376</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>-0.336375</td>\n",
       "      <td>-0.019857</td>\n",
       "      <td>0.319115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROP</th>\n",
       "      <td>-0.031847</td>\n",
       "      <td>-0.078123</td>\n",
       "      <td>-0.274776</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>0.061482</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>0.004839</td>\n",
       "      <td>-0.058474</td>\n",
       "      <td>0.034934</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058482</td>\n",
       "      <td>-0.058498</td>\n",
       "      <td>-0.025989</td>\n",
       "      <td>-0.037838</td>\n",
       "      <td>-0.058467</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>-0.058465</td>\n",
       "      <td>0.017402</td>\n",
       "      <td>0.037182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DCAL</th>\n",
       "      <td>0.253296</td>\n",
       "      <td>-0.215753</td>\n",
       "      <td>-0.143624</td>\n",
       "      <td>-0.245064</td>\n",
       "      <td>0.911076</td>\n",
       "      <td>0.211634</td>\n",
       "      <td>0.178310</td>\n",
       "      <td>0.055512</td>\n",
       "      <td>0.291568</td>\n",
       "      <td>-0.093328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287960</td>\n",
       "      <td>0.291523</td>\n",
       "      <td>-0.099243</td>\n",
       "      <td>0.358258</td>\n",
       "      <td>0.291377</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.291375</td>\n",
       "      <td>0.059265</td>\n",
       "      <td>-0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DRHO</th>\n",
       "      <td>0.249273</td>\n",
       "      <td>0.055254</td>\n",
       "      <td>0.090255</td>\n",
       "      <td>-0.256565</td>\n",
       "      <td>0.281891</td>\n",
       "      <td>0.083668</td>\n",
       "      <td>0.172003</td>\n",
       "      <td>0.106038</td>\n",
       "      <td>0.882447</td>\n",
       "      <td>-0.026357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882056</td>\n",
       "      <td>0.882461</td>\n",
       "      <td>-0.097750</td>\n",
       "      <td>0.414080</td>\n",
       "      <td>0.882169</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.882166</td>\n",
       "      <td>0.036976</td>\n",
       "      <td>-0.116644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MUDWEIGHT</th>\n",
       "      <td>-0.182790</td>\n",
       "      <td>-0.333802</td>\n",
       "      <td>-0.738548</td>\n",
       "      <td>0.193531</td>\n",
       "      <td>0.172453</td>\n",
       "      <td>0.202040</td>\n",
       "      <td>-0.063313</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>-0.062072</td>\n",
       "      <td>-0.119144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058653</td>\n",
       "      <td>-0.062000</td>\n",
       "      <td>-0.087257</td>\n",
       "      <td>-0.005210</td>\n",
       "      <td>-0.061885</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.001124</td>\n",
       "      <td>-0.061885</td>\n",
       "      <td>-0.061223</td>\n",
       "      <td>0.125224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMIC</th>\n",
       "      <td>0.146459</td>\n",
       "      <td>0.223622</td>\n",
       "      <td>0.059190</td>\n",
       "      <td>-0.150293</td>\n",
       "      <td>0.095982</td>\n",
       "      <td>0.166244</td>\n",
       "      <td>0.075098</td>\n",
       "      <td>0.022012</td>\n",
       "      <td>0.084565</td>\n",
       "      <td>0.153378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080279</td>\n",
       "      <td>0.084487</td>\n",
       "      <td>0.011413</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.084397</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>-0.000824</td>\n",
       "      <td>0.084397</td>\n",
       "      <td>0.072768</td>\n",
       "      <td>-0.101801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FORCE_2020_LITHOFACIES_LITHOLOGY</th>\n",
       "      <td>-0.007422</td>\n",
       "      <td>-0.019755</td>\n",
       "      <td>-0.034079</td>\n",
       "      <td>0.019270</td>\n",
       "      <td>-0.038173</td>\n",
       "      <td>-0.041135</td>\n",
       "      <td>-0.083413</td>\n",
       "      <td>0.009211</td>\n",
       "      <td>-0.023634</td>\n",
       "      <td>0.163939</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025120</td>\n",
       "      <td>-0.023649</td>\n",
       "      <td>0.156830</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>-0.023719</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>-0.023732</td>\n",
       "      <td>0.086791</td>\n",
       "      <td>-0.100015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carbon_Index</th>\n",
       "      <td>0.252801</td>\n",
       "      <td>0.096850</td>\n",
       "      <td>0.102346</td>\n",
       "      <td>-0.262971</td>\n",
       "      <td>0.288104</td>\n",
       "      <td>0.112537</td>\n",
       "      <td>0.204782</td>\n",
       "      <td>0.114447</td>\n",
       "      <td>0.999727</td>\n",
       "      <td>-0.006083</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>-0.064837</td>\n",
       "      <td>0.462292</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>0.019290</td>\n",
       "      <td>-0.122091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized_RHOB</th>\n",
       "      <td>0.267160</td>\n",
       "      <td>0.098485</td>\n",
       "      <td>0.105965</td>\n",
       "      <td>-0.277247</td>\n",
       "      <td>0.290592</td>\n",
       "      <td>0.113915</td>\n",
       "      <td>0.204219</td>\n",
       "      <td>0.113994</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062715</td>\n",
       "      <td>0.462418</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.022306</td>\n",
       "      <td>-0.127675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized_GR</th>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.167773</td>\n",
       "      <td>0.131760</td>\n",
       "      <td>-0.043703</td>\n",
       "      <td>-0.133204</td>\n",
       "      <td>-0.063521</td>\n",
       "      <td>0.087406</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>-0.062674</td>\n",
       "      <td>0.583831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064837</td>\n",
       "      <td>-0.062715</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.029608</td>\n",
       "      <td>-0.062719</td>\n",
       "      <td>-0.039971</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>-0.062720</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>-0.183059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta_DTC</th>\n",
       "      <td>0.215984</td>\n",
       "      <td>-0.071248</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>-0.208389</td>\n",
       "      <td>0.394364</td>\n",
       "      <td>0.088760</td>\n",
       "      <td>0.350722</td>\n",
       "      <td>0.090665</td>\n",
       "      <td>0.462418</td>\n",
       "      <td>-0.074397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462292</td>\n",
       "      <td>0.462418</td>\n",
       "      <td>-0.029608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.462362</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.462344</td>\n",
       "      <td>0.059329</td>\n",
       "      <td>-0.073349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta_RHOB</th>\n",
       "      <td>0.266868</td>\n",
       "      <td>0.098435</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>-0.276951</td>\n",
       "      <td>0.290467</td>\n",
       "      <td>0.113818</td>\n",
       "      <td>0.204328</td>\n",
       "      <td>0.114003</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>-0.002677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>-0.062719</td>\n",
       "      <td>0.462362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>-0.002774</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>-0.127563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta_GR</th>\n",
       "      <td>0.000432</td>\n",
       "      <td>-0.000287</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>-0.000414</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.051292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>-0.039971</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.000555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta_DEPTH_MD</th>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>-0.002509</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>-0.001663</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>-0.002774</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002758</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>-0.000826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta_Carbon_Index</th>\n",
       "      <td>0.266868</td>\n",
       "      <td>0.098434</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>-0.276951</td>\n",
       "      <td>0.290466</td>\n",
       "      <td>0.113820</td>\n",
       "      <td>0.204323</td>\n",
       "      <td>0.114004</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>-0.002682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999323</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>-0.062720</td>\n",
       "      <td>0.462344</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>-0.002758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022264</td>\n",
       "      <td>-0.127561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GROUP_encoded</th>\n",
       "      <td>0.056807</td>\n",
       "      <td>0.094923</td>\n",
       "      <td>0.024585</td>\n",
       "      <td>-0.044176</td>\n",
       "      <td>0.055227</td>\n",
       "      <td>-0.011004</td>\n",
       "      <td>-0.031007</td>\n",
       "      <td>0.045048</td>\n",
       "      <td>0.022361</td>\n",
       "      <td>0.062451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019290</td>\n",
       "      <td>0.022306</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.059329</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.022264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FORMATION_encoded</th>\n",
       "      <td>-0.293998</td>\n",
       "      <td>-0.225151</td>\n",
       "      <td>-0.163860</td>\n",
       "      <td>0.291771</td>\n",
       "      <td>0.017484</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>-0.022347</td>\n",
       "      <td>-0.006239</td>\n",
       "      <td>-0.127749</td>\n",
       "      <td>-0.172180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122091</td>\n",
       "      <td>-0.127675</td>\n",
       "      <td>-0.183059</td>\n",
       "      <td>-0.073349</td>\n",
       "      <td>-0.127563</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>-0.000826</td>\n",
       "      <td>-0.127561</td>\n",
       "      <td>-0.025375</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  DEPTH_MD     X_LOC     Y_LOC     Z_LOC  \\\n",
       "DEPTH_MD                          1.000000 -0.081904  0.249745 -0.987890   \n",
       "X_LOC                            -0.081904  1.000000  0.416875  0.085877   \n",
       "Y_LOC                             0.249745  0.416875  1.000000 -0.259623   \n",
       "Z_LOC                            -0.987890  0.085877 -0.259623  1.000000   \n",
       "CALI                              0.242287 -0.249819 -0.148181 -0.234391   \n",
       "RSHA                              0.072961 -0.164608 -0.175650 -0.088610   \n",
       "RMED                              0.055864 -0.044303  0.109941 -0.087831   \n",
       "RDEP                              0.062862  0.054780  0.009231 -0.111010   \n",
       "RHOB                              0.267399  0.098533  0.106056 -0.277483   \n",
       "GR                                0.098282  0.231190  0.206453 -0.101529   \n",
       "NPHI                              0.414375  0.089433  0.068874 -0.411152   \n",
       "PEF                               0.201044 -0.035485  0.071257 -0.197759   \n",
       "DTC                               0.086242 -0.076769  0.030885 -0.081243   \n",
       "SP                               -0.212204 -0.121053 -0.076501  0.174611   \n",
       "BS                               -0.618097 -0.092630 -0.167017  0.615375   \n",
       "ROP                              -0.031847 -0.078123 -0.274776  0.038685   \n",
       "DCAL                              0.253296 -0.215753 -0.143624 -0.245064   \n",
       "DRHO                              0.249273  0.055254  0.090255 -0.256565   \n",
       "MUDWEIGHT                        -0.182790 -0.333802 -0.738548  0.193531   \n",
       "RMIC                              0.146459  0.223622  0.059190 -0.150293   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY -0.007422 -0.019755 -0.034079  0.019270   \n",
       "Carbon_Index                      0.252801  0.096850  0.102346 -0.262971   \n",
       "Normalized_RHOB                   0.267160  0.098485  0.105965 -0.277247   \n",
       "Normalized_GR                     0.044830  0.167773  0.131760 -0.043703   \n",
       "Delta_DTC                         0.215984 -0.071248  0.047832 -0.208389   \n",
       "Delta_RHOB                        0.266868  0.098435  0.105851 -0.276951   \n",
       "Delta_GR                          0.000432 -0.000287 -0.000184 -0.000414   \n",
       "Delta_DEPTH_MD                    0.002445  0.000974  0.002194 -0.002509   \n",
       "Delta_Carbon_Index                0.266868  0.098434  0.105851 -0.276951   \n",
       "GROUP_encoded                     0.056807  0.094923  0.024585 -0.044176   \n",
       "FORMATION_encoded                -0.293998 -0.225151 -0.163860  0.291771   \n",
       "\n",
       "                                      CALI      RSHA      RMED      RDEP  \\\n",
       "DEPTH_MD                          0.242287  0.072961  0.055864  0.062862   \n",
       "X_LOC                            -0.249819 -0.164608 -0.044303  0.054780   \n",
       "Y_LOC                            -0.148181 -0.175650  0.109941  0.009231   \n",
       "Z_LOC                            -0.234391 -0.088610 -0.087831 -0.111010   \n",
       "CALI                              1.000000  0.230859  0.200187  0.059856   \n",
       "RSHA                              0.230859  1.000000  0.038447  0.041122   \n",
       "RMED                              0.200187  0.038447  1.000000  0.093780   \n",
       "RDEP                              0.059856  0.041122  0.093780  1.000000   \n",
       "RHOB                              0.290623  0.113860  0.204210  0.113988   \n",
       "GR                               -0.131479 -0.043932  0.117150  0.009550   \n",
       "NPHI                              0.159466  0.108301  0.087206  0.086625   \n",
       "PEF                               0.253697 -0.048833  0.196880  0.014515   \n",
       "DTC                               0.336849  0.068859  0.354425  0.076234   \n",
       "SP                                0.117929  0.262062  0.248956  0.023633   \n",
       "BS                               -0.093683 -0.077242 -0.084409 -0.049338   \n",
       "ROP                               0.061482 -0.000514 -0.014198  0.004839   \n",
       "DCAL                              0.911076  0.211634  0.178310  0.055512   \n",
       "DRHO                              0.281891  0.083668  0.172003  0.106038   \n",
       "MUDWEIGHT                         0.172453  0.202040 -0.063313  0.003755   \n",
       "RMIC                              0.095982  0.166244  0.075098  0.022012   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY -0.038173 -0.041135 -0.083413  0.009211   \n",
       "Carbon_Index                      0.288104  0.112537  0.204782  0.114447   \n",
       "Normalized_RHOB                   0.290592  0.113915  0.204219  0.113994   \n",
       "Normalized_GR                    -0.133204 -0.063521  0.087406  0.003944   \n",
       "Delta_DTC                         0.394364  0.088760  0.350722  0.090665   \n",
       "Delta_RHOB                        0.290467  0.113818  0.204328  0.114003   \n",
       "Delta_GR                          0.000167 -0.000398  0.001224  0.000662   \n",
       "Delta_DEPTH_MD                    0.000338 -0.001663  0.000175  0.000325   \n",
       "Delta_Carbon_Index                0.290466  0.113820  0.204323  0.114004   \n",
       "GROUP_encoded                     0.055227 -0.011004 -0.031007  0.045048   \n",
       "FORMATION_encoded                 0.017484  0.001706 -0.022347 -0.006239   \n",
       "\n",
       "                                      RHOB        GR  ...  Carbon_Index  \\\n",
       "DEPTH_MD                          0.267399  0.098282  ...      0.252801   \n",
       "X_LOC                             0.098533  0.231190  ...      0.096850   \n",
       "Y_LOC                             0.106056  0.206453  ...      0.102346   \n",
       "Z_LOC                            -0.277483 -0.101529  ...     -0.262971   \n",
       "CALI                              0.290623 -0.131479  ...      0.288104   \n",
       "RSHA                              0.113860 -0.043932  ...      0.112537   \n",
       "RMED                              0.204210  0.117150  ...      0.204782   \n",
       "RDEP                              0.113988  0.009550  ...      0.114447   \n",
       "RHOB                              1.000000 -0.002582  ...      0.999727   \n",
       "GR                               -0.002582  1.000000  ...     -0.006083   \n",
       "NPHI                              0.474023  0.169099  ...      0.467362   \n",
       "PEF                               0.458521  0.049547  ...      0.457744   \n",
       "DTC                               0.424886 -0.077324  ...      0.428413   \n",
       "SP                                0.074638 -0.161919  ...      0.079151   \n",
       "BS                               -0.336723 -0.182325  ...     -0.327054   \n",
       "ROP                              -0.058474  0.034934  ...     -0.058482   \n",
       "DCAL                              0.291568 -0.093328  ...      0.287960   \n",
       "DRHO                              0.882447 -0.026357  ...      0.882056   \n",
       "MUDWEIGHT                        -0.062072 -0.119144  ...     -0.058653   \n",
       "RMIC                              0.084565  0.153378  ...      0.080279   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY -0.023634  0.163939  ...     -0.025120   \n",
       "Carbon_Index                      0.999727 -0.006083  ...      1.000000   \n",
       "Normalized_RHOB                   1.000000 -0.002662  ...      0.999734   \n",
       "Normalized_GR                    -0.062674  0.583831  ...     -0.064837   \n",
       "Delta_DTC                         0.462418 -0.074397  ...      0.462292   \n",
       "Delta_RHOB                        0.999583 -0.002677  ...      0.999323   \n",
       "Delta_GR                          0.000408 -0.051292  ...      0.000334   \n",
       "Delta_DEPTH_MD                    0.000908 -0.000304  ...      0.000882   \n",
       "Delta_Carbon_Index                0.999579 -0.002682  ...      0.999323   \n",
       "GROUP_encoded                     0.022361  0.062451  ...      0.019290   \n",
       "FORMATION_encoded                -0.127749 -0.172180  ...     -0.122091   \n",
       "\n",
       "                                  Normalized_RHOB  Normalized_GR  Delta_DTC  \\\n",
       "DEPTH_MD                                 0.267160       0.044830   0.215984   \n",
       "X_LOC                                    0.098485       0.167773  -0.071248   \n",
       "Y_LOC                                    0.105965       0.131760   0.047832   \n",
       "Z_LOC                                   -0.277247      -0.043703  -0.208389   \n",
       "CALI                                     0.290592      -0.133204   0.394364   \n",
       "RSHA                                     0.113915      -0.063521   0.088760   \n",
       "RMED                                     0.204219       0.087406   0.350722   \n",
       "RDEP                                     0.113994       0.003944   0.090665   \n",
       "RHOB                                     1.000000      -0.062674   0.462418   \n",
       "GR                                      -0.002662       0.583831  -0.074397   \n",
       "NPHI                                     0.473941       0.091346   0.250326   \n",
       "PEF                                      0.458447      -0.063188   0.213564   \n",
       "DTC                                      0.424933      -0.019223   0.973028   \n",
       "SP                                       0.074799      -0.089115   0.170855   \n",
       "BS                                      -0.336593      -0.086266  -0.194345   \n",
       "ROP                                     -0.058498      -0.025989  -0.037838   \n",
       "DCAL                                     0.291523      -0.099243   0.358258   \n",
       "DRHO                                     0.882461      -0.097750   0.414080   \n",
       "MUDWEIGHT                               -0.062000      -0.087257  -0.005210   \n",
       "RMIC                                     0.084487       0.011413   0.004595   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY        -0.023649       0.156830   0.005092   \n",
       "Carbon_Index                             0.999734      -0.064837   0.462292   \n",
       "Normalized_RHOB                          1.000000      -0.062715   0.462418   \n",
       "Normalized_GR                           -0.062715       1.000000  -0.029608   \n",
       "Delta_DTC                                0.462418      -0.029608   1.000000   \n",
       "Delta_RHOB                               0.999583      -0.062719   0.462362   \n",
       "Delta_GR                                 0.000407      -0.039971   0.001921   \n",
       "Delta_DEPTH_MD                           0.000908       0.000512   0.000595   \n",
       "Delta_Carbon_Index                       0.999579      -0.062720   0.462344   \n",
       "GROUP_encoded                            0.022306       0.005206   0.059329   \n",
       "FORMATION_encoded                       -0.127675      -0.183059  -0.073349   \n",
       "\n",
       "                                  Delta_RHOB  Delta_GR  Delta_DEPTH_MD  \\\n",
       "DEPTH_MD                            0.266868  0.000432        0.002445   \n",
       "X_LOC                               0.098435 -0.000287        0.000974   \n",
       "Y_LOC                               0.105851 -0.000184        0.002194   \n",
       "Z_LOC                              -0.276951 -0.000414       -0.002509   \n",
       "CALI                                0.290467  0.000167        0.000338   \n",
       "RSHA                                0.113818 -0.000398       -0.001663   \n",
       "RMED                                0.204328  0.001224        0.000175   \n",
       "RDEP                                0.114003  0.000662        0.000325   \n",
       "RHOB                                0.999583  0.000408        0.000908   \n",
       "GR                                 -0.002677 -0.051292       -0.000304   \n",
       "NPHI                                0.473603  0.000093        0.001268   \n",
       "PEF                                 0.458318  0.000100        0.002135   \n",
       "DTC                                 0.424812 -0.000272        0.000017   \n",
       "SP                                  0.074868 -0.000084       -0.002900   \n",
       "BS                                 -0.336376 -0.000202        0.001167   \n",
       "ROP                                -0.058467 -0.000208        0.000544   \n",
       "DCAL                                0.291377  0.000301        0.000455   \n",
       "DRHO                                0.882169  0.000319        0.001025   \n",
       "MUDWEIGHT                          -0.061885  0.000001       -0.001124   \n",
       "RMIC                                0.084397  0.000390       -0.000824   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY   -0.023719  0.001524        0.000752   \n",
       "Carbon_Index                        0.999323  0.000334        0.000882   \n",
       "Normalized_RHOB                     0.999583  0.000407        0.000908   \n",
       "Normalized_GR                      -0.062719 -0.039971        0.000512   \n",
       "Delta_DTC                           0.462362  0.001921        0.000595   \n",
       "Delta_RHOB                          1.000000  0.000409       -0.002774   \n",
       "Delta_GR                            0.000409  1.000000        0.009598   \n",
       "Delta_DEPTH_MD                     -0.002774  0.009598        1.000000   \n",
       "Delta_Carbon_Index                  0.999997  0.000385       -0.002758   \n",
       "GROUP_encoded                       0.022265  0.001176        0.001489   \n",
       "FORMATION_encoded                  -0.127563  0.000555       -0.000826   \n",
       "\n",
       "                                  Delta_Carbon_Index  GROUP_encoded  \\\n",
       "DEPTH_MD                                    0.266868       0.056807   \n",
       "X_LOC                                       0.098434       0.094923   \n",
       "Y_LOC                                       0.105851       0.024585   \n",
       "Z_LOC                                      -0.276951      -0.044176   \n",
       "CALI                                        0.290466       0.055227   \n",
       "RSHA                                        0.113820      -0.011004   \n",
       "RMED                                        0.204323      -0.031007   \n",
       "RDEP                                        0.114004       0.045048   \n",
       "RHOB                                        0.999579       0.022361   \n",
       "GR                                         -0.002682       0.062451   \n",
       "NPHI                                        0.473602       0.063828   \n",
       "PEF                                         0.458312       0.024317   \n",
       "DTC                                         0.424818       0.028162   \n",
       "SP                                          0.074867      -0.072891   \n",
       "BS                                         -0.336375      -0.019857   \n",
       "ROP                                        -0.058465       0.017402   \n",
       "DCAL                                        0.291375       0.059265   \n",
       "DRHO                                        0.882166       0.036976   \n",
       "MUDWEIGHT                                  -0.061885      -0.061223   \n",
       "RMIC                                        0.084397       0.072768   \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY           -0.023732       0.086791   \n",
       "Carbon_Index                                0.999323       0.019290   \n",
       "Normalized_RHOB                             0.999579       0.022306   \n",
       "Normalized_GR                              -0.062720       0.005206   \n",
       "Delta_DTC                                   0.462344       0.059329   \n",
       "Delta_RHOB                                  0.999997       0.022265   \n",
       "Delta_GR                                    0.000385       0.001176   \n",
       "Delta_DEPTH_MD                             -0.002758       0.001489   \n",
       "Delta_Carbon_Index                          1.000000       0.022264   \n",
       "GROUP_encoded                               0.022264       1.000000   \n",
       "FORMATION_encoded                          -0.127561      -0.025375   \n",
       "\n",
       "                                  FORMATION_encoded  \n",
       "DEPTH_MD                                  -0.293998  \n",
       "X_LOC                                     -0.225151  \n",
       "Y_LOC                                     -0.163860  \n",
       "Z_LOC                                      0.291771  \n",
       "CALI                                       0.017484  \n",
       "RSHA                                       0.001706  \n",
       "RMED                                      -0.022347  \n",
       "RDEP                                      -0.006239  \n",
       "RHOB                                      -0.127749  \n",
       "GR                                        -0.172180  \n",
       "NPHI                                      -0.225500  \n",
       "PEF                                       -0.007112  \n",
       "DTC                                       -0.028480  \n",
       "SP                                         0.065500  \n",
       "BS                                         0.319115  \n",
       "ROP                                        0.037182  \n",
       "DCAL                                      -0.000308  \n",
       "DRHO                                      -0.116644  \n",
       "MUDWEIGHT                                  0.125224  \n",
       "RMIC                                      -0.101801  \n",
       "FORCE_2020_LITHOFACIES_LITHOLOGY          -0.100015  \n",
       "Carbon_Index                              -0.122091  \n",
       "Normalized_RHOB                           -0.127675  \n",
       "Normalized_GR                             -0.183059  \n",
       "Delta_DTC                                 -0.073349  \n",
       "Delta_RHOB                                -0.127563  \n",
       "Delta_GR                                   0.000555  \n",
       "Delta_DEPTH_MD                            -0.000826  \n",
       "Delta_Carbon_Index                        -0.127561  \n",
       "GROUP_encoded                             -0.025375  \n",
       "FORMATION_encoded                          1.000000  \n",
       "\n",
       "[31 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lithology.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapeamento e preparação do dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação de listas e mapas para mapear os dados da coluna _FORCE_2020_LITHOFACIES_LITHOLOGY_ conforme o padrão do PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocando as classes em uma lista\n",
    "classes = [30000, 65030, 65000, 80000, 74000, 70000, 70032, 88000, 86000, 99000, 90000, 93000]\n",
    "\n",
    "# Fazendo o mapeamento das classes para indexs e vice-versa\n",
    "class_to_idx_map = {clazz: index for index, clazz in enumerate(classes)}\n",
    "\n",
    "idx_to_class_map = {value: key for key, value in class_to_idx_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o mapeamento\n",
    "df_lithology['FORCE_2020_LITHOFACIES_LITHOLOGY'] = df_lithology['FORCE_2020_LITHOFACIES_LITHOLOGY'].apply(lambda x: class_to_idx_map[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separação dos valores que serão classificados (Y) dos valores que serão usados para fazer a classificação (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_lithology['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "df_lithology_features = df_lithology.drop(columns=['FORCE_2020_LITHOFACIES_LITHOLOGY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
       "       'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
       "       'MUDWEIGHT', 'RMIC', 'Carbon_Index', 'Normalized_RHOB', 'Normalized_GR',\n",
       "       'Delta_DTC', 'Delta_RHOB', 'Delta_GR', 'Delta_DEPTH_MD',\n",
       "       'Delta_Carbon_Index', 'GROUP_encoded', 'FORMATION_encoded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lithology_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df, features, add_encoded = False):\n",
    "    \"\"\"\n",
    "    Separa e retorna as características desejadas do dataset de lithology\n",
    "\n",
    "    Arguments:\n",
    "    df -- o dataframe pandas de lithology.\n",
    "    features -- String representando o tipo de características desejadas:\n",
    "        - 'normalized': só as colunas com 'Normalized' ou 'Delta' no nome sem as colunas com 'encoded' no nome;\n",
    "        - 'normal': só as colunas sem 'Normalized' ou 'Delta' ou 'encoded' no nome;\n",
    "        - 'all': retorna o dataset inteiro\n",
    "    add_encoded -- Boolean, default False. Se True adiciona as colunas com 'encoded' no nome ao retorno. \n",
    "        Não surte efeito se o valor 'all' for informado para o parâmetro 'features'\n",
    "\n",
    "    Return:\n",
    "    df_features -- dataframe com as caracteristicas desejadas\n",
    "    \"\"\"\n",
    "    if features == 'normalized':\n",
    "        df_features = df.loc[:, ['Normalized_RHOB', 'Normalized_GR', 'Delta_DTC', 'Delta_RHOB', 'Delta_GR', 'Delta_DEPTH_MD', 'Delta_Carbon_Index']]\n",
    "    elif features == 'normal':\n",
    "        df_features = df.loc[:, ['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP', 'RHOB', 'GR', 'NPHI', 'PEF', 'DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO', 'MUDWEIGHT', 'RMIC', 'Carbon_Index']]\n",
    "    elif features == 'all':\n",
    "        df_features = df\n",
    "        \n",
    "    # Adicionar dados encoded\n",
    "    if add_encoded == True and features != 'all':\n",
    "        df_features['GROUP_encoded'] = df['GROUP_encoded']\n",
    "        df_features['FORMATION_encoded'] = df['FORMATION_encoded']\n",
    "        \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>RDEP</th>\n",
       "      <th>RHOB</th>\n",
       "      <th>GR</th>\n",
       "      <th>...</th>\n",
       "      <th>PEF</th>\n",
       "      <th>DTC</th>\n",
       "      <th>SP</th>\n",
       "      <th>BS</th>\n",
       "      <th>ROP</th>\n",
       "      <th>DCAL</th>\n",
       "      <th>DRHO</th>\n",
       "      <th>MUDWEIGHT</th>\n",
       "      <th>RMIC</th>\n",
       "      <th>Carbon_Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494.528</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.501831</td>\n",
       "      <td>19.480835</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.611410</td>\n",
       "      <td>1.798681</td>\n",
       "      <td>1.884186</td>\n",
       "      <td>80.200851</td>\n",
       "      <td>...</td>\n",
       "      <td>20.915468</td>\n",
       "      <td>161.131180</td>\n",
       "      <td>24.612379</td>\n",
       "      <td>17.5</td>\n",
       "      <td>34.636410</td>\n",
       "      <td>1.980835</td>\n",
       "      <td>-0.574928</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>24.735691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>494.680</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.653809</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.618070</td>\n",
       "      <td>1.795641</td>\n",
       "      <td>1.889794</td>\n",
       "      <td>79.262886</td>\n",
       "      <td>...</td>\n",
       "      <td>19.383013</td>\n",
       "      <td>160.603470</td>\n",
       "      <td>23.895531</td>\n",
       "      <td>17.5</td>\n",
       "      <td>34.636410</td>\n",
       "      <td>1.968800</td>\n",
       "      <td>-0.570188</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>24.492376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>494.832</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.805786</td>\n",
       "      <td>19.468800</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.626459</td>\n",
       "      <td>1.800733</td>\n",
       "      <td>1.896523</td>\n",
       "      <td>74.821999</td>\n",
       "      <td>...</td>\n",
       "      <td>22.591518</td>\n",
       "      <td>160.173615</td>\n",
       "      <td>23.916357</td>\n",
       "      <td>17.5</td>\n",
       "      <td>34.779556</td>\n",
       "      <td>1.968800</td>\n",
       "      <td>-0.574245</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>24.202299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>494.984</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-469.957794</td>\n",
       "      <td>19.459282</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.621594</td>\n",
       "      <td>1.801517</td>\n",
       "      <td>1.891913</td>\n",
       "      <td>72.878922</td>\n",
       "      <td>...</td>\n",
       "      <td>32.191910</td>\n",
       "      <td>160.149429</td>\n",
       "      <td>23.793688</td>\n",
       "      <td>17.5</td>\n",
       "      <td>39.965164</td>\n",
       "      <td>1.959282</td>\n",
       "      <td>-0.586315</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>24.400797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>495.136</td>\n",
       "      <td>437641.96875</td>\n",
       "      <td>6470972.5</td>\n",
       "      <td>-470.109772</td>\n",
       "      <td>19.453100</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>1.602679</td>\n",
       "      <td>1.795299</td>\n",
       "      <td>1.880034</td>\n",
       "      <td>71.729141</td>\n",
       "      <td>...</td>\n",
       "      <td>38.495632</td>\n",
       "      <td>160.128342</td>\n",
       "      <td>24.104078</td>\n",
       "      <td>17.5</td>\n",
       "      <td>57.483765</td>\n",
       "      <td>1.953100</td>\n",
       "      <td>-0.597914</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>24.916765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEPTH_MD         X_LOC      Y_LOC       Z_LOC       CALI   RSHA      RMED  \\\n",
       "0   494.528  437641.96875  6470972.5 -469.501831  19.480835 -999.0  1.611410   \n",
       "1   494.680  437641.96875  6470972.5 -469.653809  19.468800 -999.0  1.618070   \n",
       "2   494.832  437641.96875  6470972.5 -469.805786  19.468800 -999.0  1.626459   \n",
       "3   494.984  437641.96875  6470972.5 -469.957794  19.459282 -999.0  1.621594   \n",
       "4   495.136  437641.96875  6470972.5 -470.109772  19.453100 -999.0  1.602679   \n",
       "\n",
       "       RDEP      RHOB         GR  ...        PEF         DTC         SP    BS  \\\n",
       "0  1.798681  1.884186  80.200851  ...  20.915468  161.131180  24.612379  17.5   \n",
       "1  1.795641  1.889794  79.262886  ...  19.383013  160.603470  23.895531  17.5   \n",
       "2  1.800733  1.896523  74.821999  ...  22.591518  160.173615  23.916357  17.5   \n",
       "3  1.801517  1.891913  72.878922  ...  32.191910  160.149429  23.793688  17.5   \n",
       "4  1.795299  1.880034  71.729141  ...  38.495632  160.128342  24.104078  17.5   \n",
       "\n",
       "         ROP      DCAL      DRHO  MUDWEIGHT   RMIC  Carbon_Index  \n",
       "0  34.636410  1.980835 -0.574928     -999.0 -999.0     24.735691  \n",
       "1  34.636410  1.968800 -0.570188     -999.0 -999.0     24.492376  \n",
       "2  34.779556  1.968800 -0.574245     -999.0 -999.0     24.202299  \n",
       "3  39.965164  1.959282 -0.586315     -999.0 -999.0     24.400797  \n",
       "4  57.483765  1.953100 -0.597914     -999.0 -999.0     24.916765  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = get_features(df_lithology_features, 'normal')\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separação em grupo de treino e teste. A proporção ficou em 80% para treino e 20% para teste. O parâmetro _random_state_ foi informado como 42 para permitir avaliar se as mudanças de rede geravam alguma alteração nos resultados. O uso do parâmetro _stratify_ se faz necessário por conta do desbalanceamento do dataset. Depois de alguns testes nesse modo, o _random_state_ foi trocado para 24 e a proporção mudou, sendo, 60%-20%-20%, criando o uso de uma base de validação para verificar uma configuração escolhida durante a primeira parte de testes. Essa forma de avaliar a rede não é muito correta, visto que a base de validação deveria ter sido separada no inicio do processo e ter sido avaliada no final, com esses dois passos e a rede treinando na base completa isto pode ter levado a escolha dos melhores parâmetros só para o dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.6, random_state=24, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente a célula acima era assim:\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.6, random_state=24, stratify=y)\n",
    "```\n",
    "Baixo está a separação da base de teste e de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=24, stratify=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936408 234103\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(scaler_type):\n",
    "    \"\"\"\n",
    "    Retorna um Scaler\n",
    "\n",
    "    Arguments:\n",
    "    scaler_type -- String representando o tipo de scaler desejado:\n",
    "        - 'minmax': MinMaxScaler;\n",
    "        - 'standard': StandardScaler;\n",
    "\n",
    "    Return:\n",
    "    scaler -- um Scaler\n",
    "    \"\"\"\n",
    "    if scaler_type == 'minmax':\n",
    "        return MinMaxScaler()\n",
    "    elif scaler_type == 'standard':\n",
    "        return StandardScaler()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando uma normalização aos dados e criando os Tensors para serem usados nos treinamentos/testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = get_scaler('standard')\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_validation = scaler.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando no Padrão do Pytorch\n",
    "X_train = torch.FloatTensor(X_train).to(device)\n",
    "X_test = torch.FloatTensor(X_test).to(device)\n",
    "X_validation = torch.FloatTensor(X_validation).to(device)\n",
    "y_train = torch.LongTensor(y_train.values).to(device)\n",
    "y_test = torch.LongTensor(y_test.values).to(device)\n",
    "y_validation = torch.LongTensor(y_validation.values).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do Modelo\n",
    "\n",
    "A quantidade de neurônios nas primeiras camadas das redes via seguir o princípio deste [link](https://medium.com/fintechexplained/what-are-hidden-layers-4f54f7328263):\n",
    "\n",
    "$$ número\\ de\\ neurônios = \\frac{numero\\ de\\ exemplos\\ de\\ treino}{Fator * (Neurônios\\ de\\ entrada + Neurônios\\ de\\ saída)} $$\n",
    "\n",
    "Onde $Fator$ é um número de 1-10.\n",
    "\n",
    "A quantidade de neurônios das redes com mais de uma camada 1 camada oculta vão ser variações deste número inical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de neurônios nas camadas oculta: 530\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = int(len(y_train) / (7 * X_train.shape[1] * 12))\n",
    "print(f\"Número de neurônios nas camadas oculta: {hidden_neurons}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fórmula a cima oferece um resultado de 530 neurônios quando a proporção de treino é 80% da base original, e esse valor fica menor usando 60% desta base. Como 530 apresentaram resultados interessantes, será mantido assim para a geração dos modelos. (Comentar a célula a baixo caso deseja usar o valor calculado pela fórmula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = 530"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição dos modelos para serem valiados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSimples(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ModelSimples, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_neurons)\n",
    "        self.layer2 = nn.Linear(hidden_neurons, hidden_neurons)\n",
    "        self.out = nn.Linear(hidden_neurons, 12) # 12 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class Model2Camadas(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model2Camadas, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_neurons)\n",
    "        self.layer2 = nn.Linear(hidden_neurons, int(hidden_neurons/2))\n",
    "        self.layer3 = nn.Linear(int(hidden_neurons/2), int(hidden_neurons/2))\n",
    "        self.out = nn.Linear(int(hidden_neurons/2), 12) # 12 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class ModelDropout(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ModelDropout, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_neurons)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.layer2 = nn.Linear(hidden_neurons, int(hidden_neurons/2))\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.layer3 = nn.Linear(int(hidden_neurons/2), int(hidden_neurons/2))\n",
    "        self.out = nn.Linear(int(hidden_neurons/2), 12) # 12 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class Model3Camadas(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model3Camadas, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_neurons)\n",
    "        self.layer2 = nn.Linear(hidden_neurons, int(hidden_neurons/2))\n",
    "        self.layer3 = nn.Linear(int(hidden_neurons/2), int(hidden_neurons/4))\n",
    "        self.layer4 = nn.Linear(int(hidden_neurons/4), int(hidden_neurons/4))\n",
    "        self.out = nn.Linear(int(hidden_neurons/4), 12) # 12 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class Model4Camadas(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model4Camadas, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_neurons)\n",
    "        self.layer2 = nn.Linear(hidden_neurons, int(hidden_neurons/2))\n",
    "        self.layer3 = nn.Linear(int(hidden_neurons/2), int(hidden_neurons/4))\n",
    "        self.layer4 = nn.Linear(int(hidden_neurons/4), int(hidden_neurons/4))\n",
    "        self.layer5 = nn.Linear(int(hidden_neurons/4), int(hidden_neurons/8))\n",
    "        self.out = nn.Linear(int(hidden_neurons/8), 12) # 12 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = F.relu(self.layer5(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class ModelSimplesDropout(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ModelSimplesDropout, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_neurons)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.layer2 = nn.Linear(hidden_neurons, hidden_neurons)\n",
    "        self.out = nn.Linear(hidden_neurons, 12) # 12 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimazer(op_type, model, lr=0.05):\n",
    "    \"\"\"\n",
    "    Retorna um Optimazer do PyTorch\n",
    "\n",
    "    Arguments:\n",
    "    op_type -- String representando o tipo de Optimazer desejado:\n",
    "        - 'adam': Adam;\n",
    "        - 'sgd': SGD;\n",
    "    model -- Modelo do PyTorch utilizado\n",
    "    lr -- escalar representando a taxa de aprendizagem do modelo. Default é 0.05\n",
    "\n",
    "    Return:\n",
    "    optimazer -- retorna um Optimazer ou None\n",
    "    \"\"\"\n",
    "    if op_type == 'adam':\n",
    "        return torch.optim.Adam(params = model.parameters(), lr=lr, amsgrad=True)\n",
    "    elif op_type == 'sgd':\n",
    "        return torch.optim.SGD(params = model.parameters(), lr=lr, momentum=0.9)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treino e Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model4Camadas(\n",
       "  (layer1): Linear(in_features=21, out_features=530, bias=True)\n",
       "  (layer2): Linear(in_features=530, out_features=265, bias=True)\n",
       "  (layer3): Linear(in_features=265, out_features=132, bias=True)\n",
       "  (layer4): Linear(in_features=132, out_features=132, bias=True)\n",
       "  (layer5): Linear(in_features=132, out_features=66, bias=True)\n",
       "  (out): Linear(in_features=66, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model4Camadas(X_train.shape[1]).to(device)\n",
    "optimizer = get_optimazer('adam', model, lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_type, X_train, y_train, model, loss_fn, optimizer, n_epochs = 100, mini_batch_size = 128):\n",
    "    \"\"\"\n",
    "    Treina um Modelo do PyTorch. Durante o treinamento será exibida a loss.\n",
    "\n",
    "    Arguments:\n",
    "    train_type -- String representando o tipo de treino desejado:\n",
    "        - 'normal_batch': treino usando os dados todos de uma vez só;\n",
    "        - 'mini_batch': treina utilizando a tecnica de mini batch.\n",
    "    X_train -- Tensor com as caracteristicas usadas para a predição.\n",
    "    y_train -- Tensor com as labels verdadeiras para as caracteristicas de 'X_train'.\n",
    "    model -- Modelo a ser teinado.\n",
    "    loss_fn -- função de loss.\n",
    "    optimizer -- Optimizer a ser utilizado.\n",
    "    n_epochs -- número de épocas para treinar. Default é 100.\n",
    "    mini_batch_size -- tamanho dos batchs para serem usados se a opção 'train_type' for 'mini_batch'. Default é 128.\n",
    "\n",
    "    Return:\n",
    "    losses -- vetor com a loss do modelo durante as épocas\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    if train_type == 'mini_batch':\n",
    "        training_set = TensorDataset(X_train, y_train)\n",
    "        trainloader = DataLoader(dataset=training_set,batch_size=mini_batch_size)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            for local_batch, local_labels in trainloader:\n",
    "                y_hat = model(local_batch)\n",
    "                loss = loss_fn(y_hat, local_labels)\n",
    "\n",
    "                #backward pass (calcular gradientes)\n",
    "                loss.backward()\n",
    "\n",
    "                #update (atualizar os pesos)\n",
    "                optimizer.step()\n",
    "\n",
    "                #limpar o otimizador\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            losses.append(loss)\n",
    "            print(f'epoch: {epoch:2}  loss: {loss.item():10.8f}')   \n",
    "            \n",
    "    elif train_type == 'normal_batch':\n",
    "        for i in range(n_epochs):\n",
    "            y_hat = model.forward(X_train)\n",
    "            loss = loss_fn(y_hat, y_train)\n",
    "            losses.append(loss)\n",
    "            print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n",
    "\n",
    "            #backward pass (calcular gradientes)\n",
    "            loss.backward()\n",
    "\n",
    "            #update (atualizar os pesos)\n",
    "            optimizer.step()\n",
    "\n",
    "            #limpar o otimizador\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss: 0.54894125\n",
      "epoch:  1  loss: 0.49264413\n",
      "epoch:  2  loss: 0.44727311\n",
      "epoch:  3  loss: 0.40870810\n",
      "epoch:  4  loss: 0.38540098\n",
      "epoch:  5  loss: 0.36844054\n",
      "epoch:  6  loss: 0.35997707\n",
      "epoch:  7  loss: 0.33463928\n",
      "epoch:  8  loss: 0.34970629\n",
      "epoch:  9  loss: 0.33127192\n",
      "epoch: 10  loss: 0.34219986\n",
      "epoch: 11  loss: 0.33950698\n",
      "epoch: 12  loss: 0.32575881\n",
      "epoch: 13  loss: 0.31778994\n",
      "epoch: 14  loss: 0.31699011\n",
      "epoch: 15  loss: 0.31482393\n",
      "epoch: 16  loss: 0.30367318\n",
      "epoch: 17  loss: 0.30132061\n",
      "epoch: 18  loss: 0.28995764\n",
      "epoch: 19  loss: 0.29654455\n",
      "epoch: 20  loss: 0.29324189\n",
      "epoch: 21  loss: 0.28870416\n",
      "epoch: 22  loss: 0.27815422\n",
      "epoch: 23  loss: 0.26935530\n",
      "epoch: 24  loss: 0.27608529\n",
      "epoch: 25  loss: 0.28026420\n",
      "epoch: 26  loss: 0.28289893\n",
      "epoch: 27  loss: 0.28175375\n",
      "epoch: 28  loss: 0.27519014\n",
      "epoch: 29  loss: 0.28033757\n",
      "epoch: 30  loss: 0.26428944\n",
      "epoch: 31  loss: 0.27316841\n",
      "epoch: 32  loss: 0.26703820\n",
      "epoch: 33  loss: 0.26026410\n",
      "epoch: 34  loss: 0.27786031\n",
      "epoch: 35  loss: 0.26170751\n",
      "epoch: 36  loss: 0.26307571\n",
      "epoch: 37  loss: 0.25419495\n",
      "epoch: 38  loss: 0.25630808\n",
      "epoch: 39  loss: 0.26321411\n",
      "epoch: 40  loss: 0.25615886\n",
      "epoch: 41  loss: 0.24327458\n",
      "epoch: 42  loss: 0.26050541\n",
      "epoch: 43  loss: 0.25293779\n",
      "epoch: 44  loss: 0.26836255\n",
      "epoch: 45  loss: 0.24504198\n",
      "epoch: 46  loss: 0.26317880\n",
      "epoch: 47  loss: 0.23127897\n",
      "epoch: 48  loss: 0.23341146\n",
      "epoch: 49  loss: 0.24460346\n",
      "epoch: 50  loss: 0.24370401\n",
      "epoch: 51  loss: 0.23494676\n",
      "epoch: 52  loss: 0.22216958\n",
      "epoch: 53  loss: 0.22154462\n",
      "epoch: 54  loss: 0.21698259\n",
      "epoch: 55  loss: 0.21082576\n",
      "epoch: 56  loss: 0.22579205\n",
      "epoch: 57  loss: 0.25462958\n",
      "epoch: 58  loss: 0.22517556\n",
      "epoch: 59  loss: 0.22705597\n",
      "epoch: 60  loss: 0.20439693\n",
      "epoch: 61  loss: 0.24296688\n",
      "epoch: 62  loss: 0.23010232\n",
      "epoch: 63  loss: 0.22140400\n",
      "epoch: 64  loss: 0.23146668\n",
      "epoch: 65  loss: 0.22946301\n",
      "epoch: 66  loss: 0.21652110\n",
      "epoch: 67  loss: 0.22434817\n",
      "epoch: 68  loss: 0.20134017\n",
      "epoch: 69  loss: 0.20825714\n",
      "epoch: 70  loss: 0.20620161\n",
      "epoch: 71  loss: 0.23058511\n",
      "epoch: 72  loss: 0.22213568\n",
      "epoch: 73  loss: 0.22303388\n",
      "epoch: 74  loss: 0.20478569\n",
      "epoch: 75  loss: 0.21974491\n",
      "epoch: 76  loss: 0.21851018\n",
      "epoch: 77  loss: 0.22700296\n",
      "epoch: 78  loss: 0.21355569\n",
      "epoch: 79  loss: 0.21856557\n",
      "epoch: 80  loss: 0.20317787\n",
      "epoch: 81  loss: 0.21397781\n",
      "epoch: 82  loss: 0.22515927\n",
      "epoch: 83  loss: 0.21154544\n",
      "epoch: 84  loss: 0.20841134\n",
      "epoch: 85  loss: 0.22201176\n",
      "epoch: 86  loss: 0.21194589\n",
      "epoch: 87  loss: 0.23453109\n",
      "epoch: 88  loss: 0.21356395\n",
      "epoch: 89  loss: 0.24492447\n",
      "epoch: 90  loss: 0.21523756\n",
      "epoch: 91  loss: 0.21000160\n",
      "epoch: 92  loss: 0.23149262\n",
      "epoch: 93  loss: 0.23497237\n",
      "epoch: 94  loss: 0.21429965\n",
      "epoch: 95  loss: 0.22921737\n",
      "epoch: 96  loss: 0.23656882\n",
      "epoch: 97  loss: 0.23520829\n",
      "epoch: 98  loss: 0.21521871\n",
      "epoch: 99  loss: 0.21736646\n",
      "epoch: 100  loss: 0.20097648\n",
      "epoch: 101  loss: 0.20461434\n",
      "epoch: 102  loss: 0.22846583\n",
      "epoch: 103  loss: 0.22872724\n",
      "epoch: 104  loss: 0.23331772\n",
      "epoch: 105  loss: 0.22769932\n",
      "epoch: 106  loss: 0.22586776\n",
      "epoch: 107  loss: 0.23614964\n",
      "epoch: 108  loss: 0.23552497\n",
      "epoch: 109  loss: 0.21982978\n",
      "epoch: 110  loss: 0.21107583\n",
      "epoch: 111  loss: 0.23258565\n",
      "epoch: 112  loss: 0.21426517\n",
      "epoch: 113  loss: 0.21692792\n",
      "epoch: 114  loss: 0.22587006\n",
      "epoch: 115  loss: 0.23631090\n",
      "epoch: 116  loss: 0.23006289\n",
      "epoch: 117  loss: 0.20758376\n",
      "epoch: 118  loss: 0.22223341\n",
      "epoch: 119  loss: 0.23113988\n",
      "epoch: 120  loss: 0.22602664\n",
      "epoch: 121  loss: 0.21628700\n",
      "epoch: 122  loss: 0.21366540\n",
      "epoch: 123  loss: 0.23085678\n",
      "epoch: 124  loss: 0.21396841\n",
      "epoch: 125  loss: 0.20711276\n",
      "epoch: 126  loss: 0.22071093\n",
      "epoch: 127  loss: 0.20849487\n",
      "epoch: 128  loss: 0.20688790\n",
      "epoch: 129  loss: 0.20255354\n",
      "epoch: 130  loss: 0.19870876\n",
      "epoch: 131  loss: 0.20814668\n",
      "epoch: 132  loss: 0.20411152\n",
      "epoch: 133  loss: 0.19513315\n",
      "epoch: 134  loss: 0.19863236\n",
      "epoch: 135  loss: 0.20955074\n",
      "epoch: 136  loss: 0.22454570\n",
      "epoch: 137  loss: 0.20635706\n",
      "epoch: 138  loss: 0.20959552\n",
      "epoch: 139  loss: 0.20353413\n",
      "epoch: 140  loss: 0.19714740\n",
      "epoch: 141  loss: 0.21038154\n",
      "epoch: 142  loss: 0.19059691\n",
      "epoch: 143  loss: 0.20932427\n",
      "epoch: 144  loss: 0.20756632\n",
      "epoch: 145  loss: 0.19663547\n",
      "epoch: 146  loss: 0.22292008\n",
      "epoch: 147  loss: 0.21220760\n",
      "epoch: 148  loss: 0.19329646\n",
      "epoch: 149  loss: 0.21712798\n",
      "epoch: 150  loss: 0.20780869\n",
      "epoch: 151  loss: 0.19956410\n",
      "epoch: 152  loss: 0.20352401\n",
      "epoch: 153  loss: 0.20624603\n",
      "epoch: 154  loss: 0.20461613\n",
      "epoch: 155  loss: 0.19935611\n",
      "epoch: 156  loss: 0.20199297\n",
      "epoch: 157  loss: 0.18950269\n",
      "epoch: 158  loss: 0.18902270\n",
      "epoch: 159  loss: 0.20913444\n",
      "epoch: 160  loss: 0.21304201\n",
      "epoch: 161  loss: 0.19517605\n",
      "epoch: 162  loss: 0.22512783\n",
      "epoch: 163  loss: 0.19436575\n",
      "epoch: 164  loss: 0.22636594\n",
      "epoch: 165  loss: 0.20015624\n",
      "epoch: 166  loss: 0.18773536\n",
      "epoch: 167  loss: 0.19084263\n",
      "epoch: 168  loss: 0.23031755\n",
      "epoch: 169  loss: 0.22066155\n",
      "epoch: 170  loss: 0.20523772\n",
      "epoch: 171  loss: 0.19576441\n",
      "epoch: 172  loss: 0.20804541\n",
      "epoch: 173  loss: 0.19195555\n",
      "epoch: 174  loss: 0.19511348\n",
      "epoch: 175  loss: 0.20834871\n",
      "epoch: 176  loss: 0.20290595\n",
      "epoch: 177  loss: 0.17472215\n",
      "epoch: 178  loss: 0.18210685\n",
      "epoch: 179  loss: 0.18303140\n",
      "epoch: 180  loss: 0.22062768\n",
      "epoch: 181  loss: 0.18281397\n",
      "epoch: 182  loss: 0.18805182\n",
      "epoch: 183  loss: 0.19242238\n",
      "epoch: 184  loss: 0.19103184\n",
      "epoch: 185  loss: 0.17134354\n",
      "epoch: 186  loss: 0.18652627\n",
      "epoch: 187  loss: 0.17938307\n",
      "epoch: 188  loss: 0.18494555\n",
      "epoch: 189  loss: 0.17996940\n",
      "epoch: 190  loss: 0.18796533\n",
      "epoch: 191  loss: 0.17617080\n",
      "epoch: 192  loss: 0.19085610\n",
      "epoch: 193  loss: 0.20535693\n",
      "epoch: 194  loss: 0.20240982\n",
      "epoch: 195  loss: 0.19361585\n",
      "epoch: 196  loss: 0.18657610\n",
      "epoch: 197  loss: 0.19799702\n",
      "epoch: 198  loss: 0.18656413\n",
      "epoch: 199  loss: 0.19249210\n",
      "epoch: 200  loss: 0.20120792\n",
      "epoch: 201  loss: 0.18300734\n",
      "epoch: 202  loss: 0.17968522\n",
      "epoch: 203  loss: 0.18948837\n",
      "epoch: 204  loss: 0.18494521\n",
      "epoch: 205  loss: 0.18371342\n",
      "epoch: 206  loss: 0.15694660\n",
      "epoch: 207  loss: 0.18859677\n",
      "epoch: 208  loss: 0.18341511\n",
      "epoch: 209  loss: 0.17322317\n",
      "epoch: 210  loss: 0.17822216\n",
      "epoch: 211  loss: 0.18451139\n",
      "epoch: 212  loss: 0.17851163\n",
      "epoch: 213  loss: 0.18266907\n",
      "epoch: 214  loss: 0.17634046\n",
      "epoch: 215  loss: 0.18779589\n",
      "epoch: 216  loss: 0.22559905\n",
      "epoch: 217  loss: 0.18199690\n",
      "epoch: 218  loss: 0.18696292\n",
      "epoch: 219  loss: 0.17936051\n",
      "epoch: 220  loss: 0.18225059\n",
      "epoch: 221  loss: 0.16655639\n",
      "epoch: 222  loss: 0.17658874\n",
      "epoch: 223  loss: 0.18590616\n",
      "epoch: 224  loss: 0.18734387\n",
      "epoch: 225  loss: 0.18171032\n",
      "epoch: 226  loss: 0.17735878\n",
      "epoch: 227  loss: 0.18008904\n",
      "epoch: 228  loss: 0.15843639\n",
      "epoch: 229  loss: 0.15935360\n",
      "epoch: 230  loss: 0.18252113\n",
      "epoch: 231  loss: 0.18160380\n",
      "epoch: 232  loss: 0.14910156\n",
      "epoch: 233  loss: 0.17157522\n",
      "epoch: 234  loss: 0.16061543\n",
      "epoch: 235  loss: 0.17961064\n",
      "epoch: 236  loss: 0.16540873\n",
      "epoch: 237  loss: 0.16668797\n",
      "epoch: 238  loss: 0.16157438\n",
      "epoch: 239  loss: 0.16640523\n",
      "epoch: 240  loss: 0.17253791\n",
      "epoch: 241  loss: 0.18088143\n",
      "epoch: 242  loss: 0.16548176\n",
      "epoch: 243  loss: 0.17758504\n",
      "epoch: 244  loss: 0.15488228\n",
      "epoch: 245  loss: 0.14920403\n",
      "epoch: 246  loss: 0.14192331\n",
      "epoch: 247  loss: 0.16735415\n",
      "epoch: 248  loss: 0.18515955\n",
      "epoch: 249  loss: 0.15328082\n",
      "epoch: 250  loss: 0.17111742\n",
      "epoch: 251  loss: 0.16931264\n",
      "epoch: 252  loss: 0.14191377\n",
      "epoch: 253  loss: 0.14996321\n",
      "epoch: 254  loss: 0.16863284\n",
      "epoch: 255  loss: 0.14543311\n",
      "epoch: 256  loss: 0.15663084\n",
      "epoch: 257  loss: 0.15755573\n",
      "epoch: 258  loss: 0.16318174\n",
      "epoch: 259  loss: 0.14903378\n",
      "epoch: 260  loss: 0.13865446\n",
      "epoch: 261  loss: 0.15921512\n",
      "epoch: 262  loss: 0.16898018\n",
      "epoch: 263  loss: 0.17381127\n",
      "epoch: 264  loss: 0.16940008\n",
      "epoch: 265  loss: 0.16320956\n",
      "epoch: 266  loss: 0.16325498\n",
      "epoch: 267  loss: 0.15649910\n",
      "epoch: 268  loss: 0.17095381\n",
      "epoch: 269  loss: 0.17575453\n",
      "epoch: 270  loss: 0.16540153\n",
      "epoch: 271  loss: 0.16633426\n",
      "epoch: 272  loss: 0.16366610\n",
      "epoch: 273  loss: 0.15485081\n",
      "epoch: 274  loss: 0.16346158\n",
      "epoch: 275  loss: 0.17341362\n",
      "epoch: 276  loss: 0.15781441\n",
      "epoch: 277  loss: 0.14375062\n",
      "epoch: 278  loss: 0.14581592\n",
      "epoch: 279  loss: 0.16618562\n",
      "epoch: 280  loss: 0.17116091\n",
      "epoch: 281  loss: 0.15339047\n",
      "epoch: 282  loss: 0.15652071\n",
      "epoch: 283  loss: 0.16490480\n",
      "epoch: 284  loss: 0.15279721\n",
      "epoch: 285  loss: 0.16691215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 286  loss: 0.16354136\n",
      "epoch: 287  loss: 0.16467424\n",
      "epoch: 288  loss: 0.16946298\n",
      "epoch: 289  loss: 0.18330783\n",
      "epoch: 290  loss: 0.16152215\n",
      "epoch: 291  loss: 0.15789855\n",
      "epoch: 292  loss: 0.15834673\n",
      "epoch: 293  loss: 0.18184505\n",
      "epoch: 294  loss: 0.13266003\n",
      "epoch: 295  loss: 0.14586434\n",
      "epoch: 296  loss: 0.16317101\n",
      "epoch: 297  loss: 0.13378721\n",
      "epoch: 298  loss: 0.14809872\n",
      "epoch: 299  loss: 0.16332859\n",
      "epoch: 300  loss: 0.15372580\n",
      "epoch: 301  loss: 0.14873023\n",
      "epoch: 302  loss: 0.15168089\n",
      "epoch: 303  loss: 0.16162336\n",
      "epoch: 304  loss: 0.13041805\n",
      "epoch: 305  loss: 0.15200642\n",
      "epoch: 306  loss: 0.14983097\n",
      "epoch: 307  loss: 0.17271985\n",
      "epoch: 308  loss: 0.15482980\n",
      "epoch: 309  loss: 0.12481748\n",
      "epoch: 310  loss: 0.14194311\n",
      "epoch: 311  loss: 0.14491357\n",
      "epoch: 312  loss: 0.14664820\n",
      "epoch: 313  loss: 0.14625481\n",
      "epoch: 314  loss: 0.14414977\n",
      "epoch: 315  loss: 0.13021930\n",
      "epoch: 316  loss: 0.15636952\n",
      "epoch: 317  loss: 0.15287787\n",
      "epoch: 318  loss: 0.12899102\n",
      "epoch: 319  loss: 0.15361020\n",
      "epoch: 320  loss: 0.13316426\n",
      "epoch: 321  loss: 0.15794377\n",
      "epoch: 322  loss: 0.15705691\n",
      "epoch: 323  loss: 0.14186765\n",
      "epoch: 324  loss: 0.14418598\n",
      "epoch: 325  loss: 0.13261984\n",
      "epoch: 326  loss: 0.14851969\n",
      "epoch: 327  loss: 0.13518037\n",
      "epoch: 328  loss: 0.13993458\n",
      "epoch: 329  loss: 0.15328509\n",
      "epoch: 330  loss: 0.14013089\n",
      "epoch: 331  loss: 0.12893742\n",
      "epoch: 332  loss: 0.15047833\n",
      "epoch: 333  loss: 0.13732482\n",
      "epoch: 334  loss: 0.12937693\n",
      "epoch: 335  loss: 0.16687983\n",
      "epoch: 336  loss: 0.14137332\n",
      "epoch: 337  loss: 0.15356342\n",
      "epoch: 338  loss: 0.13762404\n",
      "epoch: 339  loss: 0.12231817\n",
      "epoch: 340  loss: 0.13415952\n",
      "epoch: 341  loss: 0.11985702\n",
      "epoch: 342  loss: 0.14109866\n",
      "epoch: 343  loss: 0.15700749\n",
      "epoch: 344  loss: 0.12860340\n",
      "epoch: 345  loss: 0.13413939\n",
      "epoch: 346  loss: 0.15207274\n",
      "epoch: 347  loss: 0.13073879\n",
      "epoch: 348  loss: 0.13166714\n",
      "epoch: 349  loss: 0.11949009\n",
      "epoch: 350  loss: 0.12623899\n",
      "epoch: 351  loss: 0.14238343\n",
      "epoch: 352  loss: 0.15037753\n",
      "epoch: 353  loss: 0.13406083\n",
      "epoch: 354  loss: 0.11928336\n",
      "epoch: 355  loss: 0.14846833\n",
      "epoch: 356  loss: 0.15604970\n",
      "epoch: 357  loss: 0.16306894\n",
      "epoch: 358  loss: 0.14919217\n",
      "epoch: 359  loss: 0.14959466\n",
      "epoch: 360  loss: 0.16830119\n",
      "epoch: 361  loss: 0.14806359\n",
      "epoch: 362  loss: 0.13862842\n",
      "epoch: 363  loss: 0.15673432\n",
      "epoch: 364  loss: 0.12810862\n",
      "epoch: 365  loss: 0.14146097\n",
      "epoch: 366  loss: 0.12638049\n",
      "epoch: 367  loss: 0.12284449\n",
      "epoch: 368  loss: 0.12345466\n",
      "epoch: 369  loss: 0.13882394\n",
      "epoch: 370  loss: 0.15406488\n",
      "epoch: 371  loss: 0.13314150\n",
      "epoch: 372  loss: 0.11592323\n",
      "epoch: 373  loss: 0.14272597\n",
      "epoch: 374  loss: 0.14159647\n",
      "epoch: 375  loss: 0.15711389\n",
      "epoch: 376  loss: 0.13668080\n",
      "epoch: 377  loss: 0.13485417\n",
      "epoch: 378  loss: 0.11718524\n",
      "epoch: 379  loss: 0.11958994\n",
      "epoch: 380  loss: 0.15835689\n",
      "epoch: 381  loss: 0.14134710\n",
      "epoch: 382  loss: 0.12125640\n",
      "epoch: 383  loss: 0.13749714\n",
      "epoch: 384  loss: 0.13706286\n",
      "epoch: 385  loss: 0.14870878\n",
      "epoch: 386  loss: 0.12505163\n",
      "epoch: 387  loss: 0.14940886\n",
      "epoch: 388  loss: 0.15276635\n",
      "epoch: 389  loss: 0.15609251\n",
      "epoch: 390  loss: 0.15930931\n",
      "epoch: 391  loss: 0.11667871\n",
      "epoch: 392  loss: 0.13642603\n",
      "epoch: 393  loss: 0.14999582\n",
      "epoch: 394  loss: 0.13087678\n",
      "epoch: 395  loss: 0.13416474\n",
      "epoch: 396  loss: 0.14126846\n",
      "epoch: 397  loss: 0.13326961\n",
      "epoch: 398  loss: 0.13387954\n",
      "epoch: 399  loss: 0.13943024\n",
      "epoch: 400  loss: 0.13509320\n",
      "epoch: 401  loss: 0.12154989\n",
      "epoch: 402  loss: 0.12895676\n",
      "epoch: 403  loss: 0.14900871\n",
      "epoch: 404  loss: 0.14367534\n",
      "epoch: 405  loss: 0.13190940\n",
      "epoch: 406  loss: 0.13613124\n",
      "epoch: 407  loss: 0.14067571\n",
      "epoch: 408  loss: 0.11789506\n",
      "epoch: 409  loss: 0.12922807\n",
      "epoch: 410  loss: 0.11545203\n",
      "epoch: 411  loss: 0.13161455\n",
      "epoch: 412  loss: 0.12996255\n",
      "epoch: 413  loss: 0.12897384\n",
      "epoch: 414  loss: 0.14967000\n",
      "epoch: 415  loss: 0.13601749\n",
      "epoch: 416  loss: 0.13108584\n",
      "epoch: 417  loss: 0.12332777\n",
      "epoch: 418  loss: 0.14128666\n",
      "epoch: 419  loss: 0.15906887\n",
      "epoch: 420  loss: 0.14790751\n",
      "epoch: 421  loss: 0.13055405\n",
      "epoch: 422  loss: 0.13465606\n",
      "epoch: 423  loss: 0.14173569\n",
      "epoch: 424  loss: 0.13036151\n",
      "epoch: 425  loss: 0.11493539\n",
      "epoch: 426  loss: 0.11300679\n",
      "epoch: 427  loss: 0.11660818\n",
      "epoch: 428  loss: 0.12880938\n",
      "epoch: 429  loss: 0.13481405\n",
      "epoch: 430  loss: 0.14541493\n",
      "epoch: 431  loss: 0.11633952\n",
      "epoch: 432  loss: 0.13714188\n",
      "epoch: 433  loss: 0.13725784\n",
      "epoch: 434  loss: 0.13459425\n",
      "epoch: 435  loss: 0.13932516\n",
      "epoch: 436  loss: 0.13081129\n",
      "epoch: 437  loss: 0.10451233\n",
      "epoch: 438  loss: 0.13765489\n",
      "epoch: 439  loss: 0.11305670\n",
      "epoch: 440  loss: 0.11569881\n",
      "epoch: 441  loss: 0.12610704\n",
      "epoch: 442  loss: 0.13507444\n",
      "epoch: 443  loss: 0.12553389\n",
      "epoch: 444  loss: 0.15039638\n",
      "epoch: 445  loss: 0.14954762\n",
      "epoch: 446  loss: 0.13089564\n",
      "epoch: 447  loss: 0.13614057\n",
      "epoch: 448  loss: 0.12101310\n",
      "epoch: 449  loss: 0.11703812\n",
      "epoch: 450  loss: 0.13038671\n",
      "epoch: 451  loss: 0.13889398\n",
      "epoch: 452  loss: 0.13125749\n",
      "epoch: 453  loss: 0.13191344\n",
      "epoch: 454  loss: 0.13460203\n",
      "epoch: 455  loss: 0.13733649\n",
      "epoch: 456  loss: 0.13225186\n",
      "epoch: 457  loss: 0.15564860\n",
      "epoch: 458  loss: 0.13069819\n",
      "epoch: 459  loss: 0.15046270\n",
      "epoch: 460  loss: 0.12286805\n",
      "epoch: 461  loss: 0.11915190\n",
      "epoch: 462  loss: 0.12814631\n",
      "epoch: 463  loss: 0.10682643\n",
      "epoch: 464  loss: 0.12608495\n",
      "epoch: 465  loss: 0.11516361\n",
      "epoch: 466  loss: 0.13904099\n",
      "epoch: 467  loss: 0.13716972\n",
      "epoch: 468  loss: 0.13498990\n",
      "epoch: 469  loss: 0.15138365\n",
      "epoch: 470  loss: 0.13402490\n",
      "epoch: 471  loss: 0.13342023\n",
      "epoch: 472  loss: 0.15351142\n",
      "epoch: 473  loss: 0.13815154\n",
      "epoch: 474  loss: 0.12959172\n",
      "epoch: 475  loss: 0.11697689\n",
      "epoch: 476  loss: 0.12918025\n",
      "epoch: 477  loss: 0.13385959\n",
      "epoch: 478  loss: 0.12672983\n",
      "epoch: 479  loss: 0.11477274\n",
      "epoch: 480  loss: 0.12059800\n",
      "epoch: 481  loss: 0.12247430\n",
      "epoch: 482  loss: 0.13436492\n",
      "epoch: 483  loss: 0.13061030\n",
      "epoch: 484  loss: 0.12232396\n",
      "epoch: 485  loss: 0.15745571\n",
      "epoch: 486  loss: 0.12565616\n",
      "epoch: 487  loss: 0.12247619\n",
      "epoch: 488  loss: 0.15046631\n",
      "epoch: 489  loss: 0.13273443\n",
      "epoch: 490  loss: 0.12041494\n",
      "epoch: 491  loss: 0.12084065\n",
      "epoch: 492  loss: 0.13707362\n",
      "epoch: 493  loss: 0.14558898\n",
      "epoch: 494  loss: 0.13435034\n",
      "epoch: 495  loss: 0.13023187\n",
      "epoch: 496  loss: 0.13000280\n",
      "epoch: 497  loss: 0.12116581\n",
      "epoch: 498  loss: 0.11989840\n",
      "epoch: 499  loss: 0.12018401\n"
     ]
    }
   ],
   "source": [
    "losses = train_model('mini_batch', X_train, y_train, model, loss_fn, optimizer, n_epochs=500, mini_batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRÁFICO DA FUNÇÃO DE CUSTO\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOTANDO O GRÁFICO DA FUNÇÃO DE CUSTO\n",
    "print(\"GRÁFICO DA FUNÇÃO DE CUSTO\")\n",
    "plt.plot(losses, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazer a predição do dataset de Teste e validar a acurácia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazer predição de teste\n",
    "with torch.no_grad():\n",
    "    predicoes = model(X_test)\n",
    "    if torch.cuda.is_available():\n",
    "        # Só trocando o local\n",
    "        predicoes = predicoes.cpu()\n",
    "    y_hat = np.argmax(predicoes, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = [1 if corr == pred else 0 for corr, pred in zip(y_test, y_hat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.9123487482664252\n"
     ]
    }
   ],
   "source": [
    "acurracy = np.sum(corrects) / len(y_test)\n",
    "print(f'Acurácia: {acurracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método `eval` coloca o modelo em um modo de avaliação, ajustando o comportamento de certas camadas, por isso ele foi usado aqui para ver o como ele influencia nos resultados da base de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicoes = model(X_validation)\n",
    "    if torch.cuda.is_available():\n",
    "        # Só trocando o local\n",
    "        predicoes = predicoes.cpu()\n",
    "    y_hat = np.argmax(predicoes, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrects = [1 if corr == pred else 0 for corr, pred in zip(y_validation, y_hat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da validação: 0.9143934410356739\n"
     ]
    }
   ],
   "source": [
    "acurracy = np.sum(corrects) / len(y_test)\n",
    "print(f'Acurácia da validação: {acurracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 16698]\n",
      " [    1 14560]\n",
      " [    2 72979]\n",
      " [    3  3272]\n",
      " [    4   155]\n",
      " [    5  5434]\n",
      " [    6  1059]\n",
      " [    7   820]\n",
      " [    8   112]\n",
      " [    9  1560]\n",
      " [   10   393]\n",
      " [   11    10]]\n"
     ]
    }
   ],
   "source": [
    "# Exibir a predição por classe, só pra garantir que não está prevendo tudo de uma classe só\n",
    "unique, counts = np.unique(y_hat.numpy(), return_counts=True)\n",
    "\n",
    "result = np.column_stack((unique, counts)) \n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de Acurácia\n",
    "\n",
    "Aqui é apresentado uma tabela com os dados de acurácia do dataset de treino (da fase de testes 80%-20% do dataser original), junto com os dados sobre o modelo, dataset usado e outras tecnicas empregadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Modelo</th>\n",
    "    <th>Dados usados</th>\n",
    "    <th>Tipo de treinamento</th>\n",
    "    <th>Outros parâmetros</th>\n",
    "    <th>Acurácia no treino</th> \n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td rowspan=\"17\">ModelSimples</td>\n",
    "    <td>dataset completo</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.6158015916071131</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas com 'Normalized' e 'Delta' no nome</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.6323028752301337</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas com 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.627527199566003</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.6158015916071131</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Aplicado: MinMaxScaler <br/>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.6505085368406215</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' no nome (colunas 'encoded' foram adicionadas)</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Aplicado: MinMaxScaler <br/>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.6158015916071131</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.7362742040896528</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam com penalização L2 (regularização) <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.7510839245972927</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' no nome (colunas 'encoded' foram adicionadas)</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.7293840745312961</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: SGD <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.7459280743946041</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam com uso de pesos pra tentar balancear<br/>Épocas: 100 <br/>Learning Rate: 0.05 <br/> Neurônios nas camadas ocultas: 176</td>\n",
    "    <td>0.5400229813372747</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam com uso de pesos pra tentar balancear<br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.5560372998210189</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 128</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.7436726569074297</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 128</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: SGD <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.9067376325805308</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 128</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam <br/>Épocas: 500 <br/>Learning Rate: 0.001</td>\n",
    "    <td>0.9177925955669086</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 256</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam <br/>Épocas: 500 <br/>Learning Rate: 0.001</td>\n",
    "    <td>0.9180955310078512</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 256</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: SGD <br/>Épocas: 500 <br/>Learning Rate: 0.001</td>\n",
    "    <td>0.9047769571513393</td>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td rowspan=\"4\">Model2Camadas</td>\n",
    "    <td>dataset completo</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05 <br/> Neurônios nas camadas ocultas foi reduzido, não atende a fórmula apresentada</td>\n",
    "    <td>0.7097346039990944</td> \n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td>dataset completo</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Optimazer: Adam com penalização L2 (regularização) <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.6932803082403899</td> \n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td>dataset completo</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Optimazer: SGD com penalização L2 (regularização) <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.7348090370477952</td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 128</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: SGD <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.9079037859403767</td> \n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td>Model3Camadas</td>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 128</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: SGD <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.9041832014113446</td>\n",
    "  </tr> \n",
    "    \n",
    "  <tr>\n",
    "    <td rowspan=\"3\">ModelDropout</td>\n",
    "    <td>dataset completo (?)</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.6158015916071131</td> \n",
    "  </tr>   \n",
    "  <tr>\n",
    "    <td>dataset completo (?)</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Optimazer: SGD com penalização L2 (regularização) <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.6980047244161758</td>\n",
    "  </tr>   \n",
    "  <tr>\n",
    "    <td>dataset completo (?)</td>\n",
    "    <td>Minibatch de tamanho 128</td>\n",
    "    <td>Optimazer: SGD com penalização L2 (regularização) <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.8864089738277596</td> \n",
    "  </tr>  \n",
    "    \n",
    "  <tr>\n",
    "    <td>ModelSimplesDropout</td>\n",
    "    <td>dataset completo (?)</td>\n",
    "    <td>Normal</td>\n",
    "    <td>Optimazer: Adam <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.9070152881423988</td> \n",
    "  </tr> \n",
    "    \n",
    "  <tr>\n",
    "    <td>Model4Camadas</td>\n",
    "    <td>dataset completo (?)</td>\n",
    "    <td>Minibatch de tamanho 128</td>\n",
    "    <td>Optimazer: SGD com penalização L2 (regularização) <br/>Épocas: 100 <br/>Learning Rate: 0.05</td>\n",
    "    <td>0.8940850822074045</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notas da tabela:**\n",
    "- Colunas com \\'?\\' se referem uma informação que talvez esteja correta, mas como os dados mais precisos só foram anotados depois do começo dos testes pode ser que esta dado não esteja totalmente correto;\n",
    "- O ModelSimples tem mais dados pois ele foi o primeiro a ser montado e sempre era o primeiro a ser testado com alguma mudança de configuração. Nem todos os modelos foram testado com todas as configurações usadas e se algum mostrase um desempenho pior com alguma configuração as vezes o resultado nem era anotado;\n",
    "- Foi tentado anotar de cada teste, sempre presando manter informações sobre os que tiveram uma acurácia maior, sendo assim pode ser que alguma informações estejam com algum dado não totalmente correto, pela falta de anotação. Se desejar verificar, é só fazer o teste com as informações apresentadas.\n",
    "- Os números de neurônios geralmente seguiam a fórmula: \n",
    "$$ número\\ de\\ neurônios = \\frac{numero\\ de\\ exemplos\\ de\\ treino}{Fator * (Neurônios\\ de\\ entrada + Neurônios\\ de\\ saída)} $$\n",
    "\n",
    "Contudo as vezes se o número fosse muito grande (> 700) ele era dividido por 3 até ficar entre 500~700. Se nos dados da tabela não estiver dito o número, é para ser o número obtido pela fórmula. Caso durante os testes exista alguma diferença pode ser em decorrencia do número não ter sido anotado e ele ser diferente do resultado da fórmula. Note que o valor de $Fator$ pode influenciar no número de neurônios sugerido;\n",
    "- Alguns parâmetros usados nos _optimazers_ podem não ter sido anotados e estarem diferentes da última versão do notebook, visto que eles podemter sido incluidos ou removidos, isso também pode influenciar em algumas diferenças caso o teste seja rodado novamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tabela a baixo foi obtida usando a separação de 60%-20%-20% do dataset original, usando `StandardScaler` e `Adam`. Basicamente foram o que se considerou os melhores parâmetros encontrados durante os testes com o dataset dividido em 80%-20%. Também foi aplicado o método `eval` depois do teste para realizar a validação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Modelo</th>\n",
    "    <th>Dados usados</th>\n",
    "    <th>Tipo de treinamento</th>\n",
    "    <th>Outros parâmetros</th>\n",
    "    <th>Acurácia no treino</th> \n",
    "    <th>Acurácia na validação</th>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td>ModelSimples</td>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 256</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam <br/>Épocas: 500 <br/>Learning Rate: 0.001 <br/>Neurônios nas camadas ocultas: 530</td>\n",
    "    <td>0.9094753568957121</td>    \n",
    "    <td>0.9087833508470666</td>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td>Model2Camadas</td>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 256</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam <br/>Épocas: 500 <br/>Learning Rate: 0.001 <br/>Neurônios nas camadas ocultas: 530 e suas variações</td>\n",
    "    <td>0.9120639721147192</td>    \n",
    "    <td>0.9116425034101944</td>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td>Model3Camadas</td>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 256</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam <br/>Épocas: 500 <br/>Learning Rate: 0.001 <br/>Neurônios nas camadas ocultas: 530 e suas variações</td>\n",
    "    <td>0.9100591480067093</td>    \n",
    "    <td>0.909515225556951</td>\n",
    "  </tr> \n",
    "    \n",
    "  <tr>\n",
    "    <td>ModelDropout</td>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 256</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam <br/>Épocas: 500 <br/>Learning Rate: 0.001 <br/>Neurônios nas camadas ocultas: 530 e suas variações</td>\n",
    "    <td>0.9017977918457196</td> \n",
    "    <td>0.9140517096536268</td>\n",
    "  </tr>  \n",
    "    \n",
    "  <tr>\n",
    "    <td>ModelSimplesDropout</td>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 256</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam <br/>Épocas: 500 <br/>Learning Rate: 0.001 <br/>Neurônios nas camadas ocultas: 530 e suas variações</td>\n",
    "    <td>0.9073708611346052</td> \n",
    "    <td>0.9143934410356739</td>\n",
    "  </tr> \n",
    "    \n",
    "  <tr>\n",
    "    <td>Model4Camadas</td>\n",
    "    <td>dataset compostos colunas sem 'Normalized', 'Delta' e 'encoded' no nome</td>\n",
    "    <td>Minibatch de tamanho 256</td>\n",
    "    <td>Aplicado: StandardScaler <br/>Optimazer: Adam <br/>Épocas: 500 <br/>Learning Rate: 0.001 <br/>Neurônios nas camadas ocultas: 530 e suas variações</td>\n",
    "    <td>0.9123487482664252</td> \n",
    "    <td>0.9143934410356739</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo Escolhido\n",
    "\n",
    "Com base nos testes com a base divida em 80%-20% o modelo escolhido foi: o ModelSimples com 530 neurônios, com Adam, taxa de aprendizagem em 0.001, minibatch de 128 e treinado por 500 épocas. Já com base nos testes usando a base dividia em 60%-20%-20% e usando o método `eval` o modelo selecionado foi o Model2Camadas com 530 neurônios, com Adam, taxa de aprendizagem em 0.001, minibatch de 256 e treinado por 500 épocas.\n",
    "\n",
    "Ambos os modelos serão treinados nesses moldes para classificar o arquivo _hidden.csv_. Os seus estados serão salvos depois do treinamento.\n",
    "\n",
    "Com relação ao _dataset_, sersão usadas só as colunas sem _Nomalized_, _Delta_ ou _encoded_ no nome. Todas as colunas usadas serão normalizadas usando um _StandardScaler_, que teve um desempenho melhor nos testes. O treino será feito com o _dataset_ completo e o scaler treinado para o dataset será usado para normalizar os dados do _hidden.csv_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro devemos fazer a separação e normalização, usando os valores de _y_ e _df_lithology_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = y.copy()\n",
    "df_feature_model = get_features(df_lithology_features, 'normal')\n",
    "\n",
    "scaler = get_scaler('standard')\n",
    "X_model = scaler.fit_transform(df_feature_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model = torch.FloatTensor(X_model).to(device)\n",
    "y_model = torch.LongTensor(y_model.values).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model2Camadas(\n",
       "  (layer1): Linear(in_features=21, out_features=530, bias=True)\n",
       "  (layer2): Linear(in_features=530, out_features=265, bias=True)\n",
       "  (layer3): Linear(in_features=265, out_features=265, bias=True)\n",
       "  (out): Linear(in_features=265, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_model = Model2Camadas(X_train.shape[1]).to(device)\n",
    "optimizer = get_optimazer('adam', prediction_model, lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "prediction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170511\n"
     ]
    }
   ],
   "source": [
    "print(len(y_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss: 1.03775609\n",
      "epoch:  1  loss: 1.55339539\n",
      "epoch:  2  loss: 1.10266387\n",
      "epoch:  3  loss: 0.77106619\n",
      "epoch:  4  loss: 0.67388922\n",
      "epoch:  5  loss: 0.67242163\n",
      "epoch:  6  loss: 0.68082166\n",
      "epoch:  7  loss: 0.62170231\n",
      "epoch:  8  loss: 0.63677168\n",
      "epoch:  9  loss: 0.78857780\n",
      "epoch: 10  loss: 0.65310657\n",
      "epoch: 11  loss: 0.59303319\n",
      "epoch: 12  loss: 0.61916339\n",
      "epoch: 13  loss: 0.59859127\n",
      "epoch: 14  loss: 0.51603335\n",
      "epoch: 15  loss: 0.55125999\n",
      "epoch: 16  loss: 0.59293246\n",
      "epoch: 17  loss: 0.61521959\n",
      "epoch: 18  loss: 0.67023200\n",
      "epoch: 19  loss: 0.58596355\n",
      "epoch: 20  loss: 0.58008355\n",
      "epoch: 21  loss: 0.58002603\n",
      "epoch: 22  loss: 0.61438763\n",
      "epoch: 23  loss: 0.63564247\n",
      "epoch: 24  loss: 0.48565236\n",
      "epoch: 25  loss: 0.56102258\n",
      "epoch: 26  loss: 0.48241800\n",
      "epoch: 27  loss: 0.54762357\n",
      "epoch: 28  loss: 0.44807541\n",
      "epoch: 29  loss: 0.56074017\n",
      "epoch: 30  loss: 0.46908256\n",
      "epoch: 31  loss: 0.57899922\n",
      "epoch: 32  loss: 0.57648379\n",
      "epoch: 33  loss: 0.55393332\n",
      "epoch: 34  loss: 0.55786389\n",
      "epoch: 35  loss: 0.65663695\n",
      "epoch: 36  loss: 0.45594972\n",
      "epoch: 37  loss: 0.51617074\n",
      "epoch: 38  loss: 0.41350502\n",
      "epoch: 39  loss: 0.36234629\n",
      "epoch: 40  loss: 0.37899327\n",
      "epoch: 41  loss: 0.46361566\n",
      "epoch: 42  loss: 0.42617750\n",
      "epoch: 43  loss: 0.46670601\n",
      "epoch: 44  loss: 0.42508003\n",
      "epoch: 45  loss: 0.32541835\n",
      "epoch: 46  loss: 0.46421486\n",
      "epoch: 47  loss: 0.27791482\n",
      "epoch: 48  loss: 0.43171242\n",
      "epoch: 49  loss: 0.30400956\n",
      "epoch: 50  loss: 0.27950048\n",
      "epoch: 51  loss: 0.42222548\n",
      "epoch: 52  loss: 0.27634230\n",
      "epoch: 53  loss: 0.33008656\n",
      "epoch: 54  loss: 0.27742767\n",
      "epoch: 55  loss: 0.34710851\n",
      "epoch: 56  loss: 0.25831509\n",
      "epoch: 57  loss: 0.27652541\n",
      "epoch: 58  loss: 0.27671570\n",
      "epoch: 59  loss: 0.27133453\n",
      "epoch: 60  loss: 0.32963556\n",
      "epoch: 61  loss: 0.29631618\n",
      "epoch: 62  loss: 0.28577736\n",
      "epoch: 63  loss: 0.27871612\n",
      "epoch: 64  loss: 0.33428860\n",
      "epoch: 65  loss: 0.31105906\n",
      "epoch: 66  loss: 0.31604853\n",
      "epoch: 67  loss: 0.26599762\n",
      "epoch: 68  loss: 0.29860535\n",
      "epoch: 69  loss: 0.27214170\n",
      "epoch: 70  loss: 0.25828320\n",
      "epoch: 71  loss: 0.28352469\n",
      "epoch: 72  loss: 0.28904840\n",
      "epoch: 73  loss: 0.23737121\n",
      "epoch: 74  loss: 0.30056700\n",
      "epoch: 75  loss: 0.24796689\n",
      "epoch: 76  loss: 0.25806814\n",
      "epoch: 77  loss: 0.27603564\n",
      "epoch: 78  loss: 0.28375116\n",
      "epoch: 79  loss: 0.32011718\n",
      "epoch: 80  loss: 0.29763946\n",
      "epoch: 81  loss: 0.33612153\n",
      "epoch: 82  loss: 0.30537969\n",
      "epoch: 83  loss: 0.45640618\n",
      "epoch: 84  loss: 0.34023979\n",
      "epoch: 85  loss: 0.24640368\n",
      "epoch: 86  loss: 0.28221366\n",
      "epoch: 87  loss: 0.28522444\n",
      "epoch: 88  loss: 0.27424634\n",
      "epoch: 89  loss: 0.25607422\n",
      "epoch: 90  loss: 0.30779448\n",
      "epoch: 91  loss: 0.25683343\n",
      "epoch: 92  loss: 0.25876331\n",
      "epoch: 93  loss: 0.28587550\n",
      "epoch: 94  loss: 0.30379596\n",
      "epoch: 95  loss: 0.31709978\n",
      "epoch: 96  loss: 0.26646793\n",
      "epoch: 97  loss: 0.29041314\n",
      "epoch: 98  loss: 0.25748602\n",
      "epoch: 99  loss: 0.32076460\n",
      "epoch: 100  loss: 0.36827555\n",
      "epoch: 101  loss: 0.27055770\n",
      "epoch: 102  loss: 0.30990642\n",
      "epoch: 103  loss: 0.25090495\n",
      "epoch: 104  loss: 0.27029121\n",
      "epoch: 105  loss: 0.26291400\n",
      "epoch: 106  loss: 0.25567672\n",
      "epoch: 107  loss: 0.23709702\n",
      "epoch: 108  loss: 0.26680401\n",
      "epoch: 109  loss: 0.25543347\n",
      "epoch: 110  loss: 0.33189741\n",
      "epoch: 111  loss: 0.26512292\n",
      "epoch: 112  loss: 0.29321805\n",
      "epoch: 113  loss: 0.29800516\n",
      "epoch: 114  loss: 0.23140441\n",
      "epoch: 115  loss: 0.24911673\n",
      "epoch: 116  loss: 0.26399279\n",
      "epoch: 117  loss: 0.24436074\n",
      "epoch: 118  loss: 0.26040867\n",
      "epoch: 119  loss: 0.31414255\n",
      "epoch: 120  loss: 0.29651442\n",
      "epoch: 121  loss: 0.25422123\n",
      "epoch: 122  loss: 0.31702298\n",
      "epoch: 123  loss: 0.33760926\n",
      "epoch: 124  loss: 0.29720962\n",
      "epoch: 125  loss: 0.25434080\n",
      "epoch: 126  loss: 0.35210374\n",
      "epoch: 127  loss: 0.25478774\n",
      "epoch: 128  loss: 0.26022646\n",
      "epoch: 129  loss: 0.24173246\n",
      "epoch: 130  loss: 0.30162320\n",
      "epoch: 131  loss: 0.38488716\n",
      "epoch: 132  loss: 0.27597770\n",
      "epoch: 133  loss: 0.24545082\n",
      "epoch: 134  loss: 0.31118545\n",
      "epoch: 135  loss: 0.41243657\n",
      "epoch: 136  loss: 0.29480547\n",
      "epoch: 137  loss: 0.26245075\n",
      "epoch: 138  loss: 0.30334380\n",
      "epoch: 139  loss: 0.30274326\n",
      "epoch: 140  loss: 0.25861919\n",
      "epoch: 141  loss: 0.25878301\n",
      "epoch: 142  loss: 0.27622041\n",
      "epoch: 143  loss: 0.29020125\n",
      "epoch: 144  loss: 0.32002559\n",
      "epoch: 145  loss: 0.24078003\n",
      "epoch: 146  loss: 0.29177150\n",
      "epoch: 147  loss: 0.32344675\n",
      "epoch: 148  loss: 0.25031519\n",
      "epoch: 149  loss: 0.27453050\n",
      "epoch: 150  loss: 0.28759098\n",
      "epoch: 151  loss: 0.33118984\n",
      "epoch: 152  loss: 0.24783143\n",
      "epoch: 153  loss: 0.26461849\n",
      "epoch: 154  loss: 0.27230212\n",
      "epoch: 155  loss: 0.42120105\n",
      "epoch: 156  loss: 0.27362207\n",
      "epoch: 157  loss: 0.26182804\n",
      "epoch: 158  loss: 0.30827054\n",
      "epoch: 159  loss: 0.26092288\n",
      "epoch: 160  loss: 0.27145341\n",
      "epoch: 161  loss: 0.23342468\n",
      "epoch: 162  loss: 0.28588465\n",
      "epoch: 163  loss: 0.23571821\n",
      "epoch: 164  loss: 0.29676348\n",
      "epoch: 165  loss: 0.32897815\n",
      "epoch: 166  loss: 0.35728544\n",
      "epoch: 167  loss: 0.23107649\n",
      "epoch: 168  loss: 0.25419256\n",
      "epoch: 169  loss: 0.26010329\n",
      "epoch: 170  loss: 0.29393768\n",
      "epoch: 171  loss: 0.23040830\n",
      "epoch: 172  loss: 0.23764284\n",
      "epoch: 173  loss: 0.30624527\n",
      "epoch: 174  loss: 0.28570083\n",
      "epoch: 175  loss: 0.30789629\n",
      "epoch: 176  loss: 0.24832952\n",
      "epoch: 177  loss: 0.23450245\n",
      "epoch: 178  loss: 0.26362598\n",
      "epoch: 179  loss: 0.22867343\n",
      "epoch: 180  loss: 0.21555580\n",
      "epoch: 181  loss: 0.25676981\n",
      "epoch: 182  loss: 0.25613549\n",
      "epoch: 183  loss: 0.26966035\n",
      "epoch: 184  loss: 0.23864003\n",
      "epoch: 185  loss: 0.25771490\n",
      "epoch: 186  loss: 0.23803073\n",
      "epoch: 187  loss: 0.28762361\n",
      "epoch: 188  loss: 0.25756016\n",
      "epoch: 189  loss: 0.27244517\n",
      "epoch: 190  loss: 0.24331279\n",
      "epoch: 191  loss: 0.26564243\n",
      "epoch: 192  loss: 0.26231557\n",
      "epoch: 193  loss: 0.26021725\n",
      "epoch: 194  loss: 0.24015956\n",
      "epoch: 195  loss: 0.26374188\n",
      "epoch: 196  loss: 0.20478544\n",
      "epoch: 197  loss: 0.25349766\n",
      "epoch: 198  loss: 0.24372412\n",
      "epoch: 199  loss: 0.24476273\n",
      "epoch: 200  loss: 0.23100978\n",
      "epoch: 201  loss: 0.25664395\n",
      "epoch: 202  loss: 0.21141703\n",
      "epoch: 203  loss: 0.23408081\n",
      "epoch: 204  loss: 0.25215688\n",
      "epoch: 205  loss: 0.20363387\n",
      "epoch: 206  loss: 0.24163619\n",
      "epoch: 207  loss: 0.24976158\n",
      "epoch: 208  loss: 0.24959350\n",
      "epoch: 209  loss: 0.26573414\n",
      "epoch: 210  loss: 0.24206080\n",
      "epoch: 211  loss: 0.24283393\n",
      "epoch: 212  loss: 0.23919164\n",
      "epoch: 213  loss: 0.21786483\n",
      "epoch: 214  loss: 0.24924913\n",
      "epoch: 215  loss: 0.27234060\n",
      "epoch: 216  loss: 0.24326967\n",
      "epoch: 217  loss: 0.22570552\n",
      "epoch: 218  loss: 0.24715720\n",
      "epoch: 219  loss: 0.27042919\n",
      "epoch: 220  loss: 0.20560677\n",
      "epoch: 221  loss: 0.32426003\n",
      "epoch: 222  loss: 0.20271005\n",
      "epoch: 223  loss: 0.23272370\n",
      "epoch: 224  loss: 0.21589774\n",
      "epoch: 225  loss: 0.20611876\n",
      "epoch: 226  loss: 0.30036619\n",
      "epoch: 227  loss: 0.27802444\n",
      "epoch: 228  loss: 0.21291156\n",
      "epoch: 229  loss: 0.20534530\n",
      "epoch: 230  loss: 0.28614128\n",
      "epoch: 231  loss: 0.24881281\n",
      "epoch: 232  loss: 0.28226769\n",
      "epoch: 233  loss: 0.20228237\n",
      "epoch: 234  loss: 0.22232720\n",
      "epoch: 235  loss: 0.26225734\n",
      "epoch: 236  loss: 0.22436072\n",
      "epoch: 237  loss: 0.22421962\n",
      "epoch: 238  loss: 0.21621770\n",
      "epoch: 239  loss: 0.26544309\n",
      "epoch: 240  loss: 0.25850028\n",
      "epoch: 241  loss: 0.21812925\n",
      "epoch: 242  loss: 0.22686391\n",
      "epoch: 243  loss: 0.27216858\n",
      "epoch: 244  loss: 0.22676566\n",
      "epoch: 245  loss: 0.24161650\n",
      "epoch: 246  loss: 0.21733351\n",
      "epoch: 247  loss: 0.20380464\n",
      "epoch: 248  loss: 0.22989592\n",
      "epoch: 249  loss: 0.18738253\n",
      "epoch: 250  loss: 0.20215158\n",
      "epoch: 251  loss: 0.21569054\n",
      "epoch: 252  loss: 0.23911913\n",
      "epoch: 253  loss: 0.23837957\n",
      "epoch: 254  loss: 0.22147770\n",
      "epoch: 255  loss: 0.19225276\n",
      "epoch: 256  loss: 0.22639410\n",
      "epoch: 257  loss: 0.21449094\n",
      "epoch: 258  loss: 0.21830516\n",
      "epoch: 259  loss: 0.23383558\n",
      "epoch: 260  loss: 0.25303844\n",
      "epoch: 261  loss: 0.21324404\n",
      "epoch: 262  loss: 0.21068624\n",
      "epoch: 263  loss: 0.24458188\n",
      "epoch: 264  loss: 0.22734292\n",
      "epoch: 265  loss: 0.20791619\n",
      "epoch: 266  loss: 0.21743460\n",
      "epoch: 267  loss: 0.31280610\n",
      "epoch: 268  loss: 0.20619906\n",
      "epoch: 269  loss: 0.23865396\n",
      "epoch: 270  loss: 0.21013224\n",
      "epoch: 271  loss: 0.23479782\n",
      "epoch: 272  loss: 0.20704903\n",
      "epoch: 273  loss: 0.21251185\n",
      "epoch: 274  loss: 0.21082135\n",
      "epoch: 275  loss: 0.21447754\n",
      "epoch: 276  loss: 0.23655735\n",
      "epoch: 277  loss: 0.25567305\n",
      "epoch: 278  loss: 0.26663113\n",
      "epoch: 279  loss: 0.21556579\n",
      "epoch: 280  loss: 0.24628101\n",
      "epoch: 281  loss: 0.20558685\n",
      "epoch: 282  loss: 0.22366481\n",
      "epoch: 283  loss: 0.24556814\n",
      "epoch: 284  loss: 0.20456778\n",
      "epoch: 285  loss: 0.21595410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 286  loss: 0.20998962\n",
      "epoch: 287  loss: 0.20060655\n",
      "epoch: 288  loss: 0.25250274\n",
      "epoch: 289  loss: 0.24091218\n",
      "epoch: 290  loss: 0.23395492\n",
      "epoch: 291  loss: 0.24467736\n",
      "epoch: 292  loss: 0.20484127\n",
      "epoch: 293  loss: 0.21082053\n",
      "epoch: 294  loss: 0.20283040\n",
      "epoch: 295  loss: 0.19801925\n",
      "epoch: 296  loss: 0.22639653\n",
      "epoch: 297  loss: 0.20745881\n",
      "epoch: 298  loss: 0.19585784\n",
      "epoch: 299  loss: 0.18161309\n",
      "epoch: 300  loss: 0.21139765\n",
      "epoch: 301  loss: 0.21339701\n",
      "epoch: 302  loss: 0.19864893\n",
      "epoch: 303  loss: 0.21996957\n",
      "epoch: 304  loss: 0.24177115\n",
      "epoch: 305  loss: 0.20108913\n",
      "epoch: 306  loss: 0.25503418\n",
      "epoch: 307  loss: 0.21579269\n",
      "epoch: 308  loss: 0.20178977\n",
      "epoch: 309  loss: 0.19507888\n",
      "epoch: 310  loss: 0.26479194\n",
      "epoch: 311  loss: 0.20955019\n",
      "epoch: 312  loss: 0.22442681\n",
      "epoch: 313  loss: 0.20294404\n",
      "epoch: 314  loss: 0.22211619\n",
      "epoch: 315  loss: 0.17217429\n",
      "epoch: 316  loss: 0.23617341\n",
      "epoch: 317  loss: 0.28000227\n",
      "epoch: 318  loss: 0.23684715\n",
      "epoch: 319  loss: 0.21760313\n",
      "epoch: 320  loss: 0.20034985\n",
      "epoch: 321  loss: 0.22557606\n",
      "epoch: 322  loss: 0.20956549\n",
      "epoch: 323  loss: 0.20162939\n",
      "epoch: 324  loss: 0.18720599\n",
      "epoch: 325  loss: 0.20183572\n",
      "epoch: 326  loss: 0.22285558\n",
      "epoch: 327  loss: 0.24714237\n",
      "epoch: 328  loss: 0.23595312\n",
      "epoch: 329  loss: 0.23467936\n",
      "epoch: 330  loss: 0.20416622\n",
      "epoch: 331  loss: 0.21934307\n",
      "epoch: 332  loss: 0.20110069\n",
      "epoch: 333  loss: 0.20326535\n",
      "epoch: 334  loss: 0.20671521\n",
      "epoch: 335  loss: 0.25338915\n",
      "epoch: 336  loss: 0.21685301\n",
      "epoch: 337  loss: 0.19469739\n",
      "epoch: 338  loss: 0.19984677\n",
      "epoch: 339  loss: 0.17794977\n",
      "epoch: 340  loss: 0.21080384\n",
      "epoch: 341  loss: 0.19604145\n",
      "epoch: 342  loss: 0.19068441\n",
      "epoch: 343  loss: 0.20787807\n",
      "epoch: 344  loss: 0.18896092\n",
      "epoch: 345  loss: 0.20560472\n",
      "epoch: 346  loss: 0.20781735\n",
      "epoch: 347  loss: 0.23394370\n",
      "epoch: 348  loss: 0.18860690\n",
      "epoch: 349  loss: 0.23883396\n",
      "epoch: 350  loss: 0.28838384\n",
      "epoch: 351  loss: 0.24157360\n",
      "epoch: 352  loss: 0.19605105\n",
      "epoch: 353  loss: 0.18190692\n",
      "epoch: 354  loss: 0.21224579\n",
      "epoch: 355  loss: 0.21637449\n",
      "epoch: 356  loss: 0.24530695\n",
      "epoch: 357  loss: 0.19522820\n",
      "epoch: 358  loss: 0.20048358\n",
      "epoch: 359  loss: 0.20964919\n",
      "epoch: 360  loss: 0.19053581\n",
      "epoch: 361  loss: 0.21847060\n",
      "epoch: 362  loss: 0.22163244\n",
      "epoch: 363  loss: 0.32643032\n",
      "epoch: 364  loss: 0.21745959\n",
      "epoch: 365  loss: 0.17758837\n",
      "epoch: 366  loss: 0.24433361\n",
      "epoch: 367  loss: 0.18789907\n",
      "epoch: 368  loss: 0.23113084\n",
      "epoch: 369  loss: 0.24982230\n",
      "epoch: 370  loss: 0.20096326\n",
      "epoch: 371  loss: 0.20746313\n",
      "epoch: 372  loss: 0.23106754\n",
      "epoch: 373  loss: 0.18841805\n",
      "epoch: 374  loss: 0.19550982\n",
      "epoch: 375  loss: 0.19926016\n",
      "epoch: 376  loss: 0.19253454\n",
      "epoch: 377  loss: 0.21408361\n",
      "epoch: 378  loss: 0.18785383\n",
      "epoch: 379  loss: 0.23369926\n",
      "epoch: 380  loss: 0.20831594\n",
      "epoch: 381  loss: 0.18576509\n",
      "epoch: 382  loss: 0.19100223\n",
      "epoch: 383  loss: 0.17072266\n",
      "epoch: 384  loss: 0.19959629\n",
      "epoch: 385  loss: 0.19379836\n",
      "epoch: 386  loss: 0.18154825\n",
      "epoch: 387  loss: 0.19041339\n",
      "epoch: 388  loss: 0.17704818\n",
      "epoch: 389  loss: 0.19753759\n",
      "epoch: 390  loss: 0.19988830\n",
      "epoch: 391  loss: 0.18048453\n",
      "epoch: 392  loss: 0.20563632\n",
      "epoch: 393  loss: 0.19900368\n",
      "epoch: 394  loss: 0.18354411\n",
      "epoch: 395  loss: 0.28696847\n",
      "epoch: 396  loss: 0.20151398\n",
      "epoch: 397  loss: 0.21891232\n",
      "epoch: 398  loss: 0.23882684\n",
      "epoch: 399  loss: 0.26114810\n",
      "epoch: 400  loss: 0.20554234\n",
      "epoch: 401  loss: 0.17275481\n",
      "epoch: 402  loss: 0.22659718\n",
      "epoch: 403  loss: 0.22850387\n",
      "epoch: 404  loss: 0.18789890\n",
      "epoch: 405  loss: 0.20575096\n",
      "epoch: 406  loss: 0.20775604\n",
      "epoch: 407  loss: 0.17336634\n",
      "epoch: 408  loss: 0.19029474\n",
      "epoch: 409  loss: 0.18126242\n",
      "epoch: 410  loss: 0.18426953\n",
      "epoch: 411  loss: 0.19989996\n",
      "epoch: 412  loss: 0.19642743\n",
      "epoch: 413  loss: 0.20734596\n",
      "epoch: 414  loss: 0.19896309\n",
      "epoch: 415  loss: 0.20525439\n",
      "epoch: 416  loss: 0.21131900\n",
      "epoch: 417  loss: 0.23239650\n",
      "epoch: 418  loss: 0.17808640\n",
      "epoch: 419  loss: 0.20215964\n",
      "epoch: 420  loss: 0.18146461\n",
      "epoch: 421  loss: 0.19204703\n",
      "epoch: 422  loss: 0.25269902\n",
      "epoch: 423  loss: 0.18609546\n",
      "epoch: 424  loss: 0.20945419\n",
      "epoch: 425  loss: 0.24276321\n",
      "epoch: 426  loss: 0.22524780\n",
      "epoch: 427  loss: 0.24370652\n",
      "epoch: 428  loss: 0.21835998\n",
      "epoch: 429  loss: 0.20236354\n",
      "epoch: 430  loss: 0.17025296\n",
      "epoch: 431  loss: 0.23552217\n",
      "epoch: 432  loss: 0.17213751\n",
      "epoch: 433  loss: 0.22050160\n",
      "epoch: 434  loss: 0.20075512\n",
      "epoch: 435  loss: 0.22703926\n",
      "epoch: 436  loss: 0.19094285\n",
      "epoch: 437  loss: 0.22183917\n",
      "epoch: 438  loss: 0.23109490\n",
      "epoch: 439  loss: 0.18159591\n",
      "epoch: 440  loss: 0.17982017\n",
      "epoch: 441  loss: 0.18526933\n",
      "epoch: 442  loss: 0.17750622\n",
      "epoch: 443  loss: 0.22528255\n",
      "epoch: 444  loss: 0.18939339\n",
      "epoch: 445  loss: 0.19537233\n",
      "epoch: 446  loss: 0.20681502\n",
      "epoch: 447  loss: 0.21345733\n",
      "epoch: 448  loss: 0.17918807\n",
      "epoch: 449  loss: 0.18783216\n",
      "epoch: 450  loss: 0.20371625\n",
      "epoch: 451  loss: 0.19821735\n",
      "epoch: 452  loss: 0.19775264\n",
      "epoch: 453  loss: 0.21275313\n",
      "epoch: 454  loss: 0.19624899\n",
      "epoch: 455  loss: 0.19304067\n",
      "epoch: 456  loss: 0.20402084\n",
      "epoch: 457  loss: 0.18427706\n",
      "epoch: 458  loss: 0.18633416\n",
      "epoch: 459  loss: 0.20671335\n",
      "epoch: 460  loss: 0.17893469\n",
      "epoch: 461  loss: 0.18551594\n",
      "epoch: 462  loss: 0.18490316\n",
      "epoch: 463  loss: 0.22417548\n",
      "epoch: 464  loss: 0.17690258\n",
      "epoch: 465  loss: 0.17992973\n",
      "epoch: 466  loss: 0.18563649\n",
      "epoch: 467  loss: 0.18173438\n",
      "epoch: 468  loss: 0.20232598\n",
      "epoch: 469  loss: 0.19201665\n",
      "epoch: 470  loss: 0.18099467\n",
      "epoch: 471  loss: 0.20291503\n",
      "epoch: 472  loss: 0.23539990\n",
      "epoch: 473  loss: 0.20805646\n",
      "epoch: 474  loss: 0.18340488\n",
      "epoch: 475  loss: 0.17703962\n",
      "epoch: 476  loss: 0.21763234\n",
      "epoch: 477  loss: 0.19189784\n",
      "epoch: 478  loss: 0.21563303\n",
      "epoch: 479  loss: 0.19112371\n",
      "epoch: 480  loss: 0.21369906\n",
      "epoch: 481  loss: 0.20953113\n",
      "epoch: 482  loss: 0.20757781\n",
      "epoch: 483  loss: 0.17655885\n",
      "epoch: 484  loss: 0.23780999\n",
      "epoch: 485  loss: 0.20419908\n",
      "epoch: 486  loss: 0.18201059\n",
      "epoch: 487  loss: 0.18945488\n",
      "epoch: 488  loss: 0.20459335\n",
      "epoch: 489  loss: 0.17528620\n",
      "epoch: 490  loss: 0.18634485\n",
      "epoch: 491  loss: 0.21418288\n",
      "epoch: 492  loss: 0.20526913\n",
      "epoch: 493  loss: 0.18259306\n",
      "epoch: 494  loss: 0.19160745\n",
      "epoch: 495  loss: 0.20269246\n",
      "epoch: 496  loss: 0.17920384\n",
      "epoch: 497  loss: 0.19031192\n",
      "epoch: 498  loss: 0.17477664\n",
      "epoch: 499  loss: 0.21658590\n"
     ]
    }
   ],
   "source": [
    "losses = train_model('mini_batch', X_model, y_model, prediction_model, loss_fn, optimizer, n_epochs=500, mini_batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o modelo pra poder usar ele depois\n",
    "\n",
    "torch.save(prediction_model.state_dict(), 'prediction_Model2Camadas.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelSimples(\n",
       "  (layer1): Linear(in_features=21, out_features=530, bias=True)\n",
       "  (layer2): Linear(in_features=530, out_features=530, bias=True)\n",
       "  (out): Linear(in_features=530, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar o modelo salvo\n",
    "# Usar isso para carregar um dos dois modelos já salvos, o padrão de nome é prediction_<nome do modelo>.pth\n",
    "prediction_model.load_state_dict(torch.load('prediction_ModelSimples.pth', map_location=device))\n",
    "prediction_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação dos dados\n",
    "\n",
    "Carregar e preparar os dados do arquivos _hidden.csv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>RDEP</th>\n",
       "      <th>RHOB</th>\n",
       "      <th>GR</th>\n",
       "      <th>...</th>\n",
       "      <th>Carbon_Index</th>\n",
       "      <th>Normalized_RHOB</th>\n",
       "      <th>Normalized_GR</th>\n",
       "      <th>Delta_DTC</th>\n",
       "      <th>Delta_RHOB</th>\n",
       "      <th>Delta_GR</th>\n",
       "      <th>Delta_DEPTH_MD</th>\n",
       "      <th>Delta_Carbon_Index</th>\n",
       "      <th>GROUP_encoded</th>\n",
       "      <th>FORMATION_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1518.280</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.241821</td>\n",
       "      <td>15.506232</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.878615</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>103.451515</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.229715</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1518.432</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.393799</td>\n",
       "      <td>18.524611</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.874237</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>94.124893</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.206369</td>\n",
       "      <td>1.616959</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>9.326622</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1518.584</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.545776</td>\n",
       "      <td>18.855669</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.869858</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>93.586487</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.205021</td>\n",
       "      <td>0.131363</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.538406</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1518.736</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.697754</td>\n",
       "      <td>19.163353</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.865479</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>91.113373</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.198831</td>\n",
       "      <td>1.637512</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>2.473114</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1518.888</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.849609</td>\n",
       "      <td>18.489744</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.849849</td>\n",
       "      <td>0.863804</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>100.228333</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.221647</td>\n",
       "      <td>0.819153</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-9.114960</td>\n",
       "      <td>0.152</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEPTH_MD      X_LOC      Y_LOC        Z_LOC       CALI   RSHA        RMED  \\\n",
       "0  1518.280  433906.75  6460000.5 -1493.241821  15.506232 -999.0 -999.000000   \n",
       "1  1518.432  433906.75  6460000.5 -1493.393799  18.524611 -999.0 -999.000000   \n",
       "2  1518.584  433906.75  6460000.5 -1493.545776  18.855669 -999.0 -999.000000   \n",
       "3  1518.736  433906.75  6460000.5 -1493.697754  19.163353 -999.0 -999.000000   \n",
       "4  1518.888  433906.75  6460000.5 -1493.849609  18.489744 -999.0    0.849849   \n",
       "\n",
       "       RDEP   RHOB          GR  ...  Carbon_Index  Normalized_RHOB  \\\n",
       "0  0.878615 -999.0  103.451515  ...        -999.0           -999.0   \n",
       "1  0.874237 -999.0   94.124893  ...        -999.0           -999.0   \n",
       "2  0.869858 -999.0   93.586487  ...        -999.0           -999.0   \n",
       "3  0.865479 -999.0   91.113373  ...        -999.0           -999.0   \n",
       "4  0.863804 -999.0  100.228333  ...        -999.0           -999.0   \n",
       "\n",
       "   Normalized_GR  Delta_DTC  Delta_RHOB  Delta_GR  Delta_DEPTH_MD  \\\n",
       "0       0.229715  -0.000000      -999.0 -0.000000           0.000   \n",
       "1       0.206369   1.616959      -999.0  9.326622           0.152   \n",
       "2       0.205021   0.131363      -999.0  0.538406           0.152   \n",
       "3       0.198831   1.637512      -999.0  2.473114           0.152   \n",
       "4       0.221647   0.819153      -999.0 -9.114960           0.152   \n",
       "\n",
       "   Delta_Carbon_Index  GROUP_encoded  FORMATION_encoded  \n",
       "0              -999.0              5                 51  \n",
       "1              -999.0              5                 51  \n",
       "2              -999.0              5                 51  \n",
       "3              -999.0              5                 51  \n",
       "4              -999.0              5                 51  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hidden = pd.read_csv('hidden.csv', sep=';')\n",
    "df_hidden.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>X_LOC</th>\n",
       "      <th>Y_LOC</th>\n",
       "      <th>Z_LOC</th>\n",
       "      <th>CALI</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RMED</th>\n",
       "      <th>RDEP</th>\n",
       "      <th>RHOB</th>\n",
       "      <th>GR</th>\n",
       "      <th>...</th>\n",
       "      <th>PEF</th>\n",
       "      <th>DTC</th>\n",
       "      <th>SP</th>\n",
       "      <th>BS</th>\n",
       "      <th>ROP</th>\n",
       "      <th>DCAL</th>\n",
       "      <th>DRHO</th>\n",
       "      <th>MUDWEIGHT</th>\n",
       "      <th>RMIC</th>\n",
       "      <th>Carbon_Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1518.280</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.241821</td>\n",
       "      <td>15.506232</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.878615</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>103.451515</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>147.043427</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>146.526276</td>\n",
       "      <td>-1.993768</td>\n",
       "      <td>0.109706</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1518.432</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.393799</td>\n",
       "      <td>18.524611</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.874237</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>94.124893</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>145.426468</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>147.605148</td>\n",
       "      <td>1.024611</td>\n",
       "      <td>-0.006418</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1518.584</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.545776</td>\n",
       "      <td>18.855669</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.869858</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>93.586487</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>145.295105</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>140.783127</td>\n",
       "      <td>1.355669</td>\n",
       "      <td>0.022769</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1518.736</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.697754</td>\n",
       "      <td>19.163353</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.865479</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>91.113373</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>143.657593</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>125.159531</td>\n",
       "      <td>1.663353</td>\n",
       "      <td>0.024972</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1518.888</td>\n",
       "      <td>433906.75</td>\n",
       "      <td>6460000.5</td>\n",
       "      <td>-1493.849609</td>\n",
       "      <td>18.489744</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0.849849</td>\n",
       "      <td>0.863804</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>100.228333</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>142.838440</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>107.576691</td>\n",
       "      <td>0.989744</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEPTH_MD      X_LOC      Y_LOC        Z_LOC       CALI   RSHA        RMED  \\\n",
       "0  1518.280  433906.75  6460000.5 -1493.241821  15.506232 -999.0 -999.000000   \n",
       "1  1518.432  433906.75  6460000.5 -1493.393799  18.524611 -999.0 -999.000000   \n",
       "2  1518.584  433906.75  6460000.5 -1493.545776  18.855669 -999.0 -999.000000   \n",
       "3  1518.736  433906.75  6460000.5 -1493.697754  19.163353 -999.0 -999.000000   \n",
       "4  1518.888  433906.75  6460000.5 -1493.849609  18.489744 -999.0    0.849849   \n",
       "\n",
       "       RDEP   RHOB          GR  ...    PEF         DTC     SP    BS  \\\n",
       "0  0.878615 -999.0  103.451515  ... -999.0  147.043427 -999.0  17.5   \n",
       "1  0.874237 -999.0   94.124893  ... -999.0  145.426468 -999.0  17.5   \n",
       "2  0.869858 -999.0   93.586487  ... -999.0  145.295105 -999.0  17.5   \n",
       "3  0.865479 -999.0   91.113373  ... -999.0  143.657593 -999.0  17.5   \n",
       "4  0.863804 -999.0  100.228333  ... -999.0  142.838440 -999.0  17.5   \n",
       "\n",
       "          ROP      DCAL      DRHO  MUDWEIGHT   RMIC  Carbon_Index  \n",
       "0  146.526276 -1.993768  0.109706     -999.0 -999.0        -999.0  \n",
       "1  147.605148  1.024611 -0.006418     -999.0 -999.0        -999.0  \n",
       "2  140.783127  1.355669  0.022769     -999.0 -999.0        -999.0  \n",
       "3  125.159531  1.663353  0.024972     -999.0 -999.0        -999.0  \n",
       "4  107.576691  0.989744  0.024527     -999.0 -999.0        -999.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hidden_model = get_features(df_hidden, 'normal')\n",
    "df_hidden_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizar o scaler já usado nos treinos\n",
    "X_hidden = scaler.transform(df_hidden_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colocar os dados no padrão do PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hidden = torch.FloatTensor(X_hidden).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realização da Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.eval()\n",
    "with torch.no_grad():\n",
    "    predicoes = prediction_model(X_hidden)\n",
    "    if torch.cuda.is_available():\n",
    "        # Só trocando o local\n",
    "        predicoes = predicoes.cpu()\n",
    "    y_hat = np.argmax(predicoes, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação do _dataframe_ com os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lithology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lithology\n",
       "0          2\n",
       "1          2\n",
       "2          2\n",
       "3          2\n",
       "4          2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hidden_lithology = pd.DataFrame(y_hat.numpy(), columns=['lithology'])\n",
    "df_hidden_lithology.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapeamento da coluna de saída para o padrão do arquivo _lithology.csv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lithology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lithology\n",
       "0      65000\n",
       "1      65000\n",
       "2      65000\n",
       "3      65000\n",
       "4      65000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hidden_lithology['lithology'] = df_hidden_lithology['lithology'].apply(lambda x: idx_to_class_map[x])\n",
    "df_hidden_lithology.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_hidden_lithology['lithology'].value_counts().plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvar o arquivo com o padrão de nome de acordo com o modelo usado para a classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hidden_lithology.to_csv('df_hidden_lithology_Model2Camadas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias de consultas e métodos:\n",
    "\n",
    "https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n",
    "\n",
    "https://www.kaggle.com/cboychinedu/starter-hole-deviation-prediction-eda\n",
    "\n",
    "https://androidkt.com/how-to-use-class-weight-in-crossentropyloss-for-an-imbalanced-dataset/\n",
    "\n",
    "https://medium.com/@ozgur.ersoz3/iris-flowers-classification-with-pytorch-cd80c8aeeb2c\n",
    "\n",
    "https://janakiev.com/blog/pytorch-iris/\n",
    "\n",
    "https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
